# Analysis

```{r setup, include=FALSE}
library(tidyverse)
library(janitor)
library(scales)
library(survey)
library(srvyr)
library(srvyrexploR)
library(censusapi)
library(flextable)
```

This section shows how to use the survey and srvyr packages to analyze complex survey data. "Complex" surveys are those with stratification and/or clustering. The packages handles weights, and adjusts statistical tests for the survey design.

## Prepare the Data

The design object specifies the sampling design, weights, and other information. We'll work with six datasets.

The survey package includes the *Student performance in California schools* data, `api`, a record of the Academic Performance Index based on standardized testing. `api` contains three datasets that illustrate design types.

-   `apisrs` is a simple random sample of (*n* = 200) schools,
-   `apistrat` is stratified sample of 3 school types (elementary, middle, high) with simple random sampling of different sizes in each stratum,
-   `apiclus2` is a two-stage cluster sample of schools within districts.

```{r}
#| code-fold: false
data(api, package = "survey")
```

Next is the American National Election Studies (ANES) dataset from the srvyrexploR package. It contain election surveys from 2020, `anes_2020`. `anes_2020` is weighted to the sample, not the population. To make population inferences, ANES recommends using the Current Population Survey (CPS) to scale the weights up to the number of non-institutional U.S. citizens aged 18 or older living in the 50 U.S. states or D.C. in March 2020.[^04-analysis-1]

[^04-analysis-1]: To pull data from the U.S. Census, visit their [API page](https://api.census.gov/data/key_signup.html) to request a key (free and fast). Create a .Renviron file in your RStudio project directory with `CENSUS_KEY="<your assigned key>"`. Make sure .Renviron is not tracked in GitHub by adding `.Renviron` to the .gitignore file.

```{r}
#| code-fold: false
data(anes_2020, package = "srvyrexploR")

# Mar 2020, state-level population estimates.
cps_state <- censusapi::getCensus(
  name = "cps/basic/mar",
  vintage = 2020,
  region = "state",
  vars = c(
    "HRMONTH", "HRYEAR4",  # month and year of interview
    "PRTAGE", "PRCITSHP",  # age and citizenship
    "PWSSWGT"  # final person-level weight.
  ),
  key = Sys.getenv("CENSUS_KEY")
)

# Age 18+ with U.S. citizenship.
target_pop <- 
  cps_state |> 
  mutate(across(everything(), as.numeric)) |>
  filter(PRTAGE >= 18, PRCITSHP %in% c(1:4)) |>
  pull(PWSSWGT) |>
  sum()

# Scale the individual person weights to the population of interest.
anes <- anes_2020 |> mutate(Weight = V200010b / sum(V200010b) * target_pop)
```

`anes_2020` contains `r comma(nrow(anes_2020), 1)` rows. Because of stratification, the rows are weighted to account for over/under representation. Column `V200010b` is the full sample weight and ranges from `r comma(min(anes_2020$V200010b), .001)` to `r comma(max(anes_2020$V200010b), .001)` with a sum of `r comma(sum(anes_2020$V200010b), 1)`. The last line of code scales the weights so that they sum to the CPS population value, `r comma(target_pop, 1)`.

Next is the Residential Energy Consumption Survey (RECS) dataset, `recs_2020`. RECS is a study that measures energy consumption and expenditure in American households.

```{r}
#| code-fold: false
data(recs_2020, package = "srvyrexploR")
```

Last is the National Health and Nutrition Examination Survey (NHANES). The survey collected 78 attributes from (*n* = 20,293) persons.

```{r}
#| code-fold: false
data(NHANESraw, package = "NHANES")

# correction to weights
NHANESraw <- NHANESraw |> mutate(WTMEC4YR = WTMEC2YR / 2) 
```

## Create the Design Object

Most analysis is performed on a `tbl_svy` survey design object. You can use `survey::svydesign()` or the `srvyr::as_survey_design()` wrapper function. The srvyr package is usually preferable because it was designed with tidy principles.

There are two parameters in `srvyr::as_survey_design()` that reference columns in the data frame.

-   Population weights, `weights`. 

  - For an SRS design, the responses are equally weighted, so the column values should just be the population size divided by the sample size.
  - For a stratified design, the values should equal the sampled fraction of the strata population.
  
-   Finite population correction, `fpc`. The FPC reduces the variance when a substantial fraction of the total population has been sampled. Set it to the stratum population size. 

  - SRS has no strata, so the column values should just be the population size.
  - Stratified designs should use the stratum population.

::: panel-tabset

### SRS

`apisrs` is a simple random sample of 200 schools from a population of 6,194 California schools. FPC column `fpc` all equal 6,194. Weights column `pw` all equal 6,194 / 200 = 30.97.

```{r}
#| code-fold: false
apisrs |> count(pw, fpc) |> knitr::kable()

apisrs_des <- as_survey_design(apisrs, weights = pw, fpc = fpc)

summary(apisrs_des)
```

### Stratified

`apistrat` is stratified on school type (`stype`): *E* = Elementary, *M* = Middle, and *H* = High School. Samples of 100/4421 (E), 50/1018 = (M), and 50/755 (H) determine the population weights, `pw = fpc / n`. Stratified designs require the `strata` parameter.

```{r}
#| code-fold: false
apistrat |> 
  count(stype, pw, fpc) |>
  mutate(`pw*n` = pw * n) |>
  adorn_totals(,,,, -pw) |>
  knitr::kable()

apistrat_des <- as_survey_design(apistrat, weights = pw, fpc = fpc, strata = stype)

summary(apistrat_des)
```

### Two-stage Cluster

`apiclus2` is a two-level cluster design. First, 40 school districts (id `dnum`) were randomly selected from the 755 districts in the state (`fpc1` = 757). Then a random sample of up to 5 schools (id `snum`) were sampled from the `fpc2` schools in the districts. Clustered designs require the cluster `ids` from largest to smallest level. `pw = fpc1 / 40`.

```{r}
apiclus2 |> count(dnum, snum, pw, fpc1, fpc2) |> 
  adorn_totals(,,,,,n) |> DT::datatable()

apiclus_design <- svydesign(
  id = ~dnum + snum, # district id + school id
  data = apiclus2, 
  weights = ~pw, 
  fpc = ~fpc1 + fpc2 # districts in state + schools in district
)

summary(apiclus_design)
```

### Stratified Cluster (ANES)

Set `nest=TRUE` to nest clusters within the strata.

```{r}
anes_des <- as_survey_design(
    anes,
    weights = Weight,
    strata = V200010d,
    ids = V200010c,
    nest = TRUE
  )

anes_des
```

### Unstratified Cluster (RECS)

```{r}
recs_des <- as_survey_rep(
  recs_2020,
  weights = NWEIGHT,
  repweights = NWEIGHT1:NWEIGHT60,
  type = "JK1",
  scale = 59 / 60,
  mse = TRUE
)

recs_des
```

### 4-stage (NHANES)

The survey used a 4-stage design: stage 0 stratified the US by geography and proportion of minority populations; stage 1 randomly selected counties within strata; stage 2 randomly selected city blocks within counties; stage 3 randomly selected households within city blocks; and stage 4 randomly selected persons within households. When there are multiple levels of clusters like this, the convention is to assign the first cluster to `ids`. Set `nest = TRUE` because the cluster ids are nested within the strata (i.e., they are not unique).

```{r}
nhanes_des <- as_survey_design(
  NHANESraw, 
  strata = SDMVSTRA, 
  ids = SDMVPSU, 
  nest = TRUE, 
  weights = WTMEC4YR
)

summary(nhanes_des)
```

Survey weights for minorities are typically lower because designers over-sample to get adequate representation. The weights sum to the sub-populations and the total population. I.e., `WTMEC4YR` = sub population size / sample size.

```{r}
NHANESraw |>
  summarize(.by = Race1, 
            `Sum(WTMEC4YR)` = sum(WTMEC4YR), 
            `Avg(WTMEC4YR)` = mean(WTMEC4YR), 
            n = n()) |>
  mutate(`Avg * n` = `Avg(WTMEC4YR)` * n) |>
  janitor::adorn_totals() |>
  gt::gt() |>
  gt::fmt_number(decimals = 0)
```

The **survey** package functions handle the survey designs and weights. The population figures from the table above could have been built with `svytable()`.

```{r}
svytable(~Race1, design = nhanes_des) %>%
  as.data.frame() %>%
  mutate(prop = Freq / sum(Freq) * 100) %>%
  arrange(desc(prop)) %>%
  adorn_totals() %>%
  flextable() %>%
  colformat_int(j = 2) %>%
  colformat_num(j = 3, suffix = "%", digits = 0)
```

:::

## Descriptive Analysis

### Distributions

Create cross-tabs with `survey_count()`. 

```{r}
apisrs_des |> survey_count(stype)

# apisrs |> 
#   summarize(.by = stype, n = sum(pw), n_se = sd(pw))
#   count(Region, Division)
```


### Central Tendency

### Relationship

### Dispersion

Create a contingency table by including two variables in `svytable()`. Here is contingency table for self-reported health by depression expressed as a 100% stacked bar chart.

```{r}
svytable(~Depressed + HealthGen, design = nhanes_des) %>%
  data.frame() %>%
  group_by(HealthGen) %>%
  mutate(n_HealthGen = sum(Freq), Prop_Depressed = Freq / sum(Freq)) %>%
  ggplot(aes(x = HealthGen, y = Prop_Depressed, fill = Depressed)) +
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  scale_fill_brewer()
```

Perform a chi-square test of independence on contingency tables using the `svychisq()` function. Here is a test ofthe null hypothesis that depression is independent of general health.

```{r}
svychisq(~Depressed + HealthGen, design = nhanes_des, statistic = "Chisq")
```

The chi-square test with Rao & Scott adjustment is evidently not a standard chi-square test. *Maybe in how it factors in survey design?* The test statistic is usually $X^2 = \sum (O - E)^2 / E.$

```{r collapse=TRUE}
O <- svytable(~Depressed + HealthGen, design = nhanes_des) %>% as.matrix()
E <- sum(O) * prop.table(O, 1) * prop.table(O, 2)
(X2 <- sum((O - E)^2 / E))
pchisq(X2, df = (nrow(O)-1) * (ncol(O) - 1), lower.tail = FALSE)
```

which is what `chisq.test()` does.

```{r}
svytable(~Depressed + HealthGen, design = nhanes_des) %>% 
  as.matrix() %>% 
  chisq.test()
```

## Exploring Quantitative Data

The `svymean()`, `svytotal()`, and `svyquantile()` functions summarize quantitative variables. To group by a factor variable, use `svyby()`.

```{r}
svyquantile(x = ~SleepHrsNight, 
            design = nhanes_des, 
            na.rm = TRUE, 
            quantiles = c(.01, .25, .50, .75, .99))

svymean(x = ~SleepHrsNight, design = nhanes_des, na.rm = TRUE)
```

```{r}
svyby(formula = ~SleepHrsNight, by = ~Depressed, FUN = svymean, 
      design = nhanes_des, na.rm = TRUE, keep.names = FALSE) %>%
  ggplot(aes(x = Depressed, y = SleepHrsNight, 
             ymin = SleepHrsNight - 2*se, ymax = SleepHrsNight + 2*se)) +
  geom_col(fill = "lightblue") +
  geom_errorbar(width = 0.5)
```

You need raw data for the distribution plots, so be sure to weight the variables.

```{r}
NHANESraw %>% 
  ggplot(aes(x = SleepHrsNight, weight = WTMEC4YR)) + 
  geom_histogram(binwidth = 1, fill = "lightblue", color = "#FFFFFF", na.rm = TRUE)

NHANESraw %>% 
  filter(!is.na(SleepHrsNight) & !is.na(Gender)) %>%
  group_by(Gender) %>%
  mutate(WTMEC4YR_std = WTMEC4YR / sum(WTMEC4YR)) %>%
  ggplot(aes(x = SleepHrsNight, Weight = WTMEC4YR_std)) +
  geom_density(bw = 0.6, fill = "lightblue") +
  labs(x = "Sleep Hours per Night") +
  facet_wrap(~Gender, labeller = "label_both")
```

Test whether the population averages differ with a two-sample survey-weighted t-test. Use the `svytest()` function to incorporate the survey design.

```{r}
svyttest(formula = SleepHrsNight ~ Gender, design = nhanes_des)
```

## Modeling Quantitative Data

Scatterplots need to adjust for the sampling weights. You can do this with the `size` or `alpha` aesthetics.

```{r}
p1 <- NHANESraw %>% 
  filter(Age == 20) %>%
  ggplot(aes(x = Height, y = Weight, color = Gender, size = WTMEC4YR)) +
  geom_jitter(width = 0.3, height = 0, alpha = 0.3) +
  guides(size = FALSE) +
  theme(legend.position = "top") +
  labs(color = "")

p2 <- NHANESraw %>% 
  filter(Age == 20) %>%
  ggplot(aes(x = Height, y = Weight, color = Gender, alpha = WTMEC4YR)) +
  geom_jitter(width = 0.3, height = 0) +
  guides(alpha = FALSE) +
  theme(legend.position = "top") +
  labs(color = "")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

Fit a regression line with `geom_smooth()`.

```{r}
NHANESraw %>% 
  filter(!is.na(Weight) & !is.na(Height)) %>%
  ggplot(aes(x = Height, y = Weight, size = WTMEC4YR)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), 
              formula = y ~ x, color = "blue") +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), 
              formula = y ~ poly(x, 2), color = "orange") +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), 
              formula = y ~ poly(x, 3), color = "red") +
guides(size = FALSE) 
```

Model a regression line with `svyglm()`. Let's build a model to predict, BPSysAve, a person's systolic blood pressure reading, using BPDiaAve, a person's diastolic blood pressure reading and Diabetes, whether or not they were diagnosed with diabetes.

```{r}
drop_na(NHANESraw, Diabetes, BPDiaAve, BPSysAve) %>%
ggplot(mapping = aes(x = BPDiaAve, y = BPSysAve, size = WTMEC4YR, color = Diabetes)) + 
    geom_point(alpha = 0.2) + 
    guides(size = FALSE) + 
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE, mapping = aes(weight = WTMEC4YR))

mod <- svyglm(BPSysAve ~ BPDiaAve*Diabetes, design = nhanes_des)
summary(mod)
```

## Survey Administration {#surveyadministration}

The second phase of a survey analysis is to collect the responses and perform an exploratory data analysis to familiarize yourself with the data.

### Frequencies

`brand_rep` is a brand reputation survey of *n* = 599 respondents answering nine 5-point Likert-scale items. The responses come in as numeric, and you will want to leave them that way for most analyses.

```{r message=FALSE}
#brand_rep <- read_csv(url("https://assets.datacamp.com/production/repositories/4494/datasets/59b5f2d717ddd647415d8c88aa40af6f89ed24df/brandrep-cleansurvey-extraitem.csv"))

brand_rep <- read_csv("input/brand_rep.csv")

psych::response.frequencies(brand_rep)
```

Summarize Likert response with the `likert::likert()` function. This is the one place where you will need the items to be treated as factors.

```{r}
brand_rep %>%
  data.frame() %>% # read_csv() returns a tibble
  mutate(across(everything(), as.factor)) %>%  # likert() uses factors
  likert::likert() %>%
  plot() + 
  labs(title = "Brand Reputation Survey") +
  theme(legend.position = "top")
```

Missing values may mean respondents did not understand the question or did not want to reveal their answer. If \<5% of survey responses have no missing values, you can just drop those responses. If missing values are a problem, try the `Hmisc::naclus()` to see which items tend to be missing in the same record. This survey is clean.

```{r collapse=TRUE}
nrow(brand_rep) - nrow(na.omit(brand_rep)) # num cases
colSums(is.na(brand_rep)) # num cases by col
```

### Correlations

You will want to identify items that correlate highly with each other, but not highly outside their group. These patterns are the basis of mapping factors to the latent variables. Factors are the concrete survey items; latent variables are the abstract concepts they are intended to supply, like *brand loyalty* or *customer satisfaction*. The correlation plot below appears to have 3 groups, plus a stand-alone variable (`one_of_a_kind`).

```{r}
#psych::corr.test(brand_rep)
corrplot::corrplot(cor(brand_rep), method = "circle")
```

## Reporting

There are seven key areas to report:

-   Explain the study objective, explicitly identifying the research question.
-   Motivate the research in the context of previous work.
-   Explain the method and rationale, including the instrument and its psychometric properties, it development/testing, sample selection, and data collection. Explain and justify the analytical methods.
-   Present the results in a concise and factual manner.
-   Interpret and discuss the findings.
-   Draw conclusions.
