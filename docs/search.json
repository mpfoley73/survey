[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survey Design and Analysis",
    "section": "",
    "text": "Preface\nThese notes are a personal reference related to my survey work. The notes primarily rely on the Applied Survey Data Analysis text by Heeringa (2017), and Exploring Complex Survey Data Analysis Using R by Zimmer (2024).\nRepo for this quarto book: https://github.com/mpfoley73/survey.\n\n\n\n\n\n\nHeeringa, West, S. G. 2017. Applied Survey Data Analysis (2nd Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9781315153278.\n\n\nZimmer, Powell, S. A. 2024. Exploring Complex Survey Data Analysis Using r: A Tidy Introduction with Srvyr and Survey. Chapman & Hall: CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-survey_projects.html",
    "href": "01-survey_projects.html",
    "title": "1  Survey Projects",
    "section": "",
    "text": "1.1 Sources of Error\nWhen planning for a new survey project, be aware of these sources of error (Zimmer 2024):",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Projects</span>"
    ]
  },
  {
    "objectID": "01-survey_projects.html#sources-of-error",
    "href": "01-survey_projects.html#sources-of-error",
    "title": "1  Survey Projects",
    "section": "",
    "text": "Coverage Error: The sampling frame from which participants are selected isn’t representative of the target population.\nSampling Error: The difference between all potential samples under the same sampling method.\nNonresponse Error: Differences between responders and non-responders to the survey (unit nonresponse) or a question (item nonresponse).\nAdjustment Error: Error introduced during post-survey statistical adjustments.\nValidity: A question doesn’t quite measure the research topic.\nMeasurement Error: An answer doesn’t quite appropriate to the question.\nProcessing Error: The researcher makes invalid edits when cleaning and coding the responses.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Projects</span>"
    ]
  },
  {
    "objectID": "01-survey_projects.html#survey-planning-document",
    "href": "01-survey_projects.html#survey-planning-document",
    "title": "1  Survey Projects",
    "section": "1.2 Survey Planning Document",
    "text": "1.2 Survey Planning Document\nA survey project will benefit from clearly thinking through the questions it aims to answer and how the survey will answer them. The sections below are a kind of project planning document template.\n\n1. Overview\n\nProject Title: [Name of the survey project].\n\n2024 Market Trends Survey\n\nStakeholders: [List all primary business groups and stakeholders].\n\nMarketing, Product Development, and Sales Teams\n\nSurvey Objective: [Outline what you aim to learn or measure with this survey].\n\nTo understand consumer interest in emerging product features to inform 2025 product line decisions.\n\nIntended Use of Survey Results: [Describe how the results will be used, e.g., market analysis, customer feedback, product development]\n\nInsights will shape product feature prioritization and guide marketing messaging for the upcoming year.\n\n\n\n\n2. Target Audience\n\nDemographics: Age, gender, location, job family, company organization, job tenure, etc.\n\nAges 25-45, primarily located in urban areas across North America.\n\nSample Size and Sampling Method: Desired number of respondents, sampling method (e.g., random, stratified, etc.). If using a stratified design, consider selection weighting responses in the analysis phase. Selection weights are usually constructed so that the sum of weights equals the population size. Weights also often account for nonresponse rates.\n\nTarget sample size of 1,000 respondents, with stratified sampling by age group and location to ensure diverse representation.\n\nCriteria for Participation: [Specific qualifications for respondents, such as existing customers, location, or other demographics].\n\nExisting customers and those who have interacted with our brand online within the past year.\n\nExpected Response Rate: [Estimate based on previous surveys or research].\n\n10-15% based on similar previous surveys.\n\n\n\n\n3. Survey Design\n\nSurvey Mode: [Indicate online, telephone, in-person, etc.].\n\nOnline (distributed via email and social media).\n\nQuestion Types: [Types of questions, e.g., multiple-choice, Likert scale, open-ended].\n\nCombination of multiple-choice, Likert scale, and a few open-ended questions for qualitative insights.\n\nEstimated Time for Completion: [Average time it should take respondents to complete the survey].\n\nApproximately 5-7 minutes\n\nQuestion Development: Provide a list of tentative questions or topics; ensure they align with objectives. It may be helpful to first outline the topics and state why each question or topic is important to the research question(s) (Zimmer 2024). To maximize the validity of the questions, consider piloting questions to a test group. Item selection is discussed more fully in Chapter 2.\n\n“How likely are you to purchase a product with [Feature X]?” (Scale: Very Unlikely to Very Likely)\n“What factors influence your decision to purchase this type of product?” (Check all that apply)\n“Please describe any features you feel are missing from our current products.” (Open-ended)\n\n\n\n\n4. Timeline and Milestones\n\nPre-survey Activities: [Date ranges for planning activities, including approvals, survey design, and testing].\n\nSurvey design completion: December 1\nStakeholder approval: December 5\nSurvey testing: December 6 - 10\n\nSurvey Launch Date: [Target launch date].\n\nDecember 15\n\nData Collection Period: [Duration for data collection, including reminders if applicable].\n\nDecember 15 - 30 (with reminder emails on December 20 and 27)\n\nAnalysis Period: [Estimate of the time required to analyze results].\n\nJanuary 2 - 10\n\nReporting Deadline: [Date to deliver insights to stakeholders].\n\nJanuary 15\n\n\n\n\n5. Distribution and Communication\n\nSurvey Platform/Tool: [Tool or software for administering the survey, e.g., Qualtrics, Google Forms].\n\nQualtrics\n\nDistribution Channels: [Email, SMS, website, in-person, etc.].\n\nEmail (main channel), with follow-up reminders, plus social media targeting for broader reach\n\nCommunications Plan: [Plan for notifying respondents about the survey, follow-ups, and thank-yous].\n\nInitial email to respondents on December 15, personalized with a greeting and clear subject line\nTwo follow-up reminders to non-responders\nThank-you message and incentive (discount code) for all participants after completion\n\n\n\n\n6. Data Handling and Privacy\n\nConfidentiality Assurance: [Details on how respondents’ information will be protected].\n\nRespondents will remain anonymous, with aggregated data only used for analysis.\n\nData Storage Plan: [Describe where the data will be stored and for how long].\n\nData will be stored on secure internal servers and deleted after 6 months.\n\nCompliance with Regulations: [Outline adherence to GDPR, CCPA, or any internal data handling policies].\n\nCompliance with GDPR for respondents in the EU, ensuring opt-in consent and secure data handling practices.\n\n\n\n\n7. Success Criteria and KPIs\n\nResponse Rate Target: [Define an acceptable response rate].\n\nAim for a 12% response rate, based on past surveys.\n\nData Quality Goals: [Define any measures for data quality, such as completeness, reliability, etc.].\n\nSeek at least 95% question completion rate and minimal drop-offs after question 3.\n\nKey Performance Indicators (KPIs): [List KPIs for measuring survey success, like completion rate, question abandonment rate].\n\nOverall response rate Average completion time (target of 5 minutes) Open-ended question completion rate for additional insights\n\n\n\n\n8. Risk Assessment\n\nPotential Challenges: [Identify challenges, such as low response rates or survey fatigue].\n\nRisk of low response rate due to survey timing near holidays\nPotential survey fatigue among respondents who have completed previous surveys this year\n\nRisk Mitigation Strategies: [Plan to address potential issues; e.g., incentives for completion, reminder schedule].\n\nOffer an incentive (discount code)\nLimit survey length to under 10 questions to maintain engagement\n\n\n\n\n9. Budget\n\nEstimated Cost Breakdown: [Include costs for software, incentives, and distribution].\n\nSurvey tool (Qualtrics): $300 \nIncentives (discount codes): $1,000\nEstimated total: $1,300\n\nApproval Status: [Is project approved or in proposal stage?].\n\nPending approval from Finance by November 30\n\n\n\n\n10. Post-Survey Plan\n\nData Cleaning and Preparation: [What data preparation tasks will be performed.].\n\nData will be cleaned for completeness, removing any responses with more than 50% skipped questions.\n\nAnalysis Strategy: [Describe methods used to analyze data.].\n\nDescriptive statistics for quantitative data (mean, median for Likert scale questions) Thematic analysis for open-ended responses to identify recurring themes\n\nReporting Format: [Markdown report, dashboard, other.].\n\nResults will be presented in a PowerPoint deck, summarizing key findings and insights, with visual charts from Excel.\n\nPresentation to Stakeholders: [Date of final deilverable.].\n\nStakeholders meeting on January 15 to discuss findings and potential next steps.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Projects</span>"
    ]
  },
  {
    "objectID": "01-survey_projects.html#documentation",
    "href": "01-survey_projects.html#documentation",
    "title": "1  Survey Projects",
    "section": "1.3 Documentation",
    "text": "1.3 Documentation\nInclude the following documentation, possibly in the the GitHub README.md file.\n\nIntroduction: Project background, survey purpose, and the main research questions.\nStudy design: How survey is prepared and administered.\nSample: Sample frame, any known sampling errors, and limitations of the sample. Instructions for using sampling weights. Population sizes, finite population correction, or replicate weight scaling information.\nFielding Notes: Additional notes on fielding, such as response rates.\n\nA questionnaire provides information about the questions posed to respondents. A codebook explains how the survey data were coded and recorded, including variable names, labels, meanings, codes for missing data, value labels, and value data types.\n\n\n\n\n\n\nZimmer, Powell, S. A. 2024. Exploring Complex Survey Data Analysis Using r: A Tidy Introduction with Srvyr and Survey. Chapman & Hall: CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Projects</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html",
    "href": "02-item_selection.html",
    "title": "2  Item Selection",
    "section": "",
    "text": "2.0.1 1. Survey Design and Item Development",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#reliability-and-validity",
    "href": "02-item_selection.html#reliability-and-validity",
    "title": "2  Item Selection",
    "section": "2.1 Reliability and Validity",
    "text": "2.1 Reliability and Validity\nA high quality survey will be both reliable (consistent) and valid (accurate).1 A reliable survey is reproducible under similar conditions. It produces consistent results across time, samples, and sub-samples within the survey itself. Reliable surveys are not necessarily accurate though. A valid survey accurately measures its latent variable. Its results are compatible with established theory and with other measures of the concept. Valid surveys are usually reliable too.\nReliability can entail one or more of the following:\n\nInter-rater reliability (aka Equivalence). Each survey item is unambiguous, so subject matter experts should respond identically. This applies to opinion surveys of factual concepts, not personal preference surveys. E.g., two psychologists completing a survey assessing a patient’s mental health should respond identically. Use the Cohen’s Kappa test of inter-rater reliability to test equivalence (see Section @ref(itemgeneration) on item generation).\nInternal consistency. Survey items measuring the same latent variable are highly correlated. Use the Cronbach alpha test and split-half test to assess internal consistency (see Section @ref(itemreduction) on item reduction).\nStability (aka Test-retest). Repeated measurements yield the same results. You use the test-retest construct to assess stability in the Replication phase.\n\nThere are three assessments of validity:\n\nContent validity. The survey items cover all aspects of the latent variable. Use Lawshe’s CVR to assess content validity (see Section @ref(itemgeneration) on item generation).\nConstruct validity. The survey items are properly grounded on a theory of the latent variable. Use convergent analysis and discriminant analysis to assess construct validity in the Convergent/Discriminant Validity phase.\nCriterion validity. The survey item results correspond to other valid measures of the same latent variable. Use concurrent analysis and predictive analysis to assess criterion validity in the Replication phase.\n\nSurvey considerations\n\nQuestion order, selection order\nrespondent burden and fatigue\n\nQuality of Measurement: Reliability and Validity (internal and external)\nPretest and pilot test\nProbability Sampling: - simple random, systematic random, stratified random, cluster, and multistate cluster. - power analysis\nReporting - Descriptive statistics - Write-up\nContinuous latent variables (e.g., level of satisfaction) can be measured with factor analysis (exploratory and confirmatory) or item response theory (IRT) models. Categorical or discrete variables (e.g., market segment) can be modeled with latent class analysis (LCA) or latent mixture modeling. You can even combine models, e.g., satisfaction within market segment.\nIn practice, you specify the model, evaluate the fit, then revise the model or add/drop items from the survey.\nA full survey project usually consists of six phases.2\n\nItem Generation (Section @ref(itemgeneration)). Start by generating a list of candidate survey items. With help from SMEs, you evaluate the equivalence (interrater reliability) and content validity of the candidate survey items and pare down the list into the final survey.\nSurvey Administration (Section @ref(surveyadministration)). Administer the survey to respondents and perform an exploratory data analysis. Summarize the Likert items with plots and look for correlations among the variables.\nItem Reduction (Section @ref(itemreduction)). Explore the dimensions of the latent variable in the survey data with parallel analysis and exploratory factor analysis. Assess the internal consistency of the items with Cronbach’s alpha and split-half tests, and remove items that do not add value and/or amend your theory of the number of dimensions.\nConfirmatory Factor Analysis (Section @ref(confirmatoryfactoranalysis)). Perform a formal hypothesis test of the theory that emerged from the exploratory factor analysis.\nConvergent/Discriminant Validity (Section @ref(convergentvalidity)). Test for convergent and discriminant construct validity.\nReplication (Section @ref(replication)). Establish test-retest reliability and criterion validity.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#itemgeneration",
    "href": "02-item_selection.html#itemgeneration",
    "title": "2  Item Selection",
    "section": "2.2 Item Generation",
    "text": "2.2 Item Generation\nDefine your latent variable(s), that is, the unquantifiable variables you intend to infer from variables you can quntify. E.g., “Importance of 401(k) matching”\nAfter you generate a list of candidate survey items, enlist SMEs to assess their inter-rater reliability with Cohen’s Kappa and content validity with Lawshe’s CVR.\n\n2.2.1 Cohen’s Kappa\nAn item has inter-rater reliability if it produces consistent results across raters. One way to test this is by having SMEs take the survey. Their answers should be close to each other. Conduct an inter-rater reliability test by measuring the statistical significance of SME response agreement using the Kohen’s kappa test statistic.\nSuppose your survey measures brand loyalty and two SMEs answer 13 survey items like this. The SMEs agreed on 6 of the 13 items (46%).\n\nsme %&gt;% mutate(agreement = RATER_A == RATER_B)\n\n   RATER_A RATER_B agreement\n1        1       1      TRUE\n2        2       2      TRUE\n3        3       2     FALSE\n4        2       3     FALSE\n5        1       3     FALSE\n6        1       1      TRUE\n7        1       1      TRUE\n8        2       1     FALSE\n9        3       2     FALSE\n10       3       3      TRUE\n11       2       3     FALSE\n12       1       3     FALSE\n13       1       1      TRUE\n\n\nYou could measure SME agreement with a simple correlation matrix (cor(sme)) or by measuring the percentage of items they rate identically (irr::agree(sme)), but these measures do not test for statistical validity.\n\ncor(sme)\n##           RATER_A   RATER_B\n## RATER_A 1.0000000 0.3291403\n## RATER_B 0.3291403 1.0000000\nirr::agree(sme)\n##  Percentage agreement (Tolerance=0)\n## \n##  Subjects = 13 \n##    Raters = 2 \n##   %-agree = 46.2\n\nInstead, calculate the Kohen’s kappa test statistic, \\(\\kappa\\), to assess statistical validity. Cohen’s kappa compares the observed agreement (accuracy) to the probability of chance agreement. \\(\\kappa\\) &gt;= 0.8 is very strong agreement, \\(\\kappa\\) &gt;= 0.6 substantial, \\(\\kappa\\) &gt;= 0.4 moderate, and \\(\\kappa\\) &lt; 0.4 is poor agreement. In this example, \\(\\kappa\\) is only 0.32 (poor agreement).\n\npsych::cohen.kappa(sme)\n\nCall: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels, \n    w.exp = w.exp)\n\nCohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries \n                 lower estimate upper\nunweighted kappa -0.18     0.19  0.55\nweighted kappa   -0.13     0.32  0.76\n\n Number of subjects = 13 \n\npsych::cohen.kappa(sme2)\n\nCall: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels, \n    w.exp = w.exp)\n\nCohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries \n                 lower estimate upper\nunweighted kappa -0.22     0.17  0.56\nweighted kappa    0.30     0.56  0.83\n\n Number of subjects = 13 \n\n\nUse the weighted kappa for ordinal measures like Likert items (see Wikipedia).\n\n\n2.2.2 Lawshe’s CVR\nAn item has content validity if SMEs agree on its relevance to the latent variable. Test content validity with Lawshe’s content validity ratio (CVR),\n\\[CVR = \\frac{E - N/2}{N/2}\\] where \\(N\\) is the number of SMEs and \\(E\\) is the number who rate the item as essential. CVR can range from -1 to 1. E.g., suppose three SMEs (A, B, and C) assess the relevance of 5 survey items as “Not Necessary”, “Useful”, or “Essential”:\n\n\n  item            A            B            C\n1    1    Essential       Useful Not necesary\n2    2       Useful Not necesary       Useful\n3    3 Not necesary Not necesary    Essential\n4    4    Essential       Useful    Essential\n5    5    Essential    Essential    Essential\n\n\nUse the psychometric::CVratio() function to calculate CVR. The threshold CVR to keep or drop an item depends on the number of raters. CVR should be &gt;= 0.99 for 5 experts; &gt;= 0.49 for 15, and &gt;= 0.29 for 40.\n\nsme2 %&gt;% \n  pivot_longer(-item, names_to = \"expert\", values_to = \"rating\") %&gt;%\n  group_by(item) %&gt;% \n  summarize(.groups = \"drop\",\n            n_sme = length(unique(expert)),\n            n_ess = sum(rating == \"Essential\"),\n            CVR = psychometric::CVratio(NTOTAL = n_sme, NESSENTIAL = n_ess))\n\n# A tibble: 5 × 4\n   item n_sme n_ess    CVR\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     1     3     1 -0.333\n2     2     3     0 -1    \n3     3     3     1 -0.333\n4     4     3     2  0.333\n5     5     3     3  1    \n\n\nIn this example, items vary widely in content validity from unanimous consensus for to unanimous consensus against.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#itemreduction",
    "href": "02-item_selection.html#itemreduction",
    "title": "2  Item Selection",
    "section": "2.3 Item Reduction",
    "text": "2.3 Item Reduction\nThe third phase, explores the mapping of the factors (aka “manifest variables”) to the latent variable’s “dimensions” and refines the survey to exclude factors that do not map to a dimension. A latent variable may have several dimensions. E.g., “brand loyalty” may consist of “brand identification”, “perceived value”, and “brand trust”. Exploratory factor analysis (EFA), identifies the dimensions in the data, and whether any items do not reveal information about the latent variable. EFA establishes the internal reliability, whether similar items produce similar scores.\nStart with a parallel analysis and scree plot. This will suggest the number of factors in the data. Use this number as the input to an exploratory factor analysis.\n\n2.3.1 Parallel Analysis\nA scree plot is a line plot of the eigenvalues. An eigenvalue is the proportion of variance explained by each factor. Only factors with eigenvalues greater than those from uncorrelated data are useful. You want to find a sharp reduction in the size of the eigenvalues (like a cliff), with the rest of the smaller eigenvalues constituting rubble (scree!). After the eigenvalues drop dramatically in size, additional factors add relatively little to the information already extracted.\nParallel analysis helps to make the interpretation of scree plots more objective. The eigenvalues are plotted along with eigenvalues of simulated variables with population correlations of 0. The number of eigenvalues above the point where the two lines intersect is the suggested number of factors. The rationale for parallel analysis is that useful factors account for more variance than could be expected by chance.\npsych::fa.parallel() compares a scree of your data set to a random data set to identify the number of factors. The elbow below here is at 3 factors.\n\nbrand_rep &lt;- read_csv(url(\"https://assets.datacamp.com/production/repositories/4494/datasets/59b5f2d717ddd647415d8c88aa40af6f89ed24df/brandrep-cleansurvey-extraitem.csv\"))\n\npsych::fa.parallel(brand_rep)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\n\n\n\n2.3.2 Exporatory Factor Analysis\nUse psych::fa() to perform the factor analysis with your chosen number of factors. The number of factors may be the result of your parallel analysis, or the opinion of the SMEs. In this case, we’ll go with the 3 factors identified by the parallel analysis.\n\nbrand_rep_efa &lt;- psych::fa(brand_rep, nfactors = 3)\n# psych::scree(brand_rep) # psych makes scree plot's too.\npsych::fa.diagram(brand_rep_efa)\n\n\n\n\n\n\n\n\nUsing EFA, you may tweak the number of factors or drop poorly-loading items. Each item should load highly to one and only one dimension. This one dimension is the item’s primary loading. Generally, a primary loading &gt; .7 is excellent, &gt;.6 is very good, &gt;.5 is good, &gt;.4 is fair, and &lt;.4 is poor. Here are the factor loadings from the 3 factor model.\n\nbrand_rep_efa$loadings\n\n\nLoadings:\n               MR2    MR1    MR3   \nwell_made       0.896              \nconsistent      0.947              \npoor_workman_r  0.739              \nhigher_price    0.127  0.632  0.149\nlot_more               0.850       \ngo_up                  0.896       \nstands_out                    1.008\nunique                        0.896\none_of_a_kind   0.309  0.295  0.115\n\n                 MR2   MR1   MR3\nSS loadings    2.361 2.012 1.858\nProportion Var 0.262 0.224 0.206\nCumulative Var 0.262 0.486 0.692\n\n\nThe brand-rep survey items load to 3 factors well except for the one_of_a_kind item. Its primary factor loading (0.309) is poor. The others are either very good (.6-.7) and excellent (&gt;.7) range.\nLook at the model eigenvalues. There should be one eigenvalue per dimension. Eigenvalues a little less than one may be contaminating the model.\n\nbrand_rep_efa$e.value\n\n[1] 4.35629549 1.75381015 0.98701607 0.67377072 0.39901205 0.35865598 0.23915591\n[8] 0.14238807 0.08989556\n\n\nLook at the factor score correlations. They should all be around 0.6. Much smaller means they are not describing the same latent variable. Much larger means they are describing the same dimension of the latent variable.\n\nbrand_rep_efa$r.scores\n\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.3147485 0.4778759\n[2,] 0.3147485 1.0000000 0.5428218\n[3,] 0.4778759 0.5428218 1.0000000\n\n\nIf you have a poorly loaded dimension, drop factors one at a time from the scale. one_of_a_kind loads across all three factors, but does not load strongly onto any one factor. one_of_a_kind is not clearly measuring any dimension of the latent variable. Drop it and try again.\n\nbrand_rep_efa &lt;- psych::fa(brand_rep %&gt;% select(-one_of_a_kind), nfactors = 3)\nbrand_rep_efa$loadings\n\n\nLoadings:\n               MR2    MR3    MR1   \nwell_made       0.887              \nconsistent      0.958              \npoor_workman_r  0.735              \nhigher_price    0.120  0.596  0.170\nlot_more               0.845       \ngo_up                  0.918       \nstands_out                    0.990\nunique                        0.916\n\n                 MR2   MR3   MR1\nSS loadings    2.261 1.915 1.850\nProportion Var 0.283 0.239 0.231\nCumulative Var 0.283 0.522 0.753\n\nbrand_rep_efa$e.value\n\n[1] 4.00884429 1.75380537 0.97785883 0.42232527 0.36214534 0.24086772 0.14404264\n[8] 0.09011054\n\nbrand_rep_efa$r.scores\n\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.2978009 0.4802138\n[2,] 0.2978009 1.0000000 0.5351654\n[3,] 0.4802138 0.5351654 1.0000000\n\n\nThis is better. We have three dimensions of brand reputation:\n\nitems well_made, consistent, and poor_workman_r describe Product Quality,\nitems higher_price, lot_more, and go_up describe Willingness to Pay, and\nitems stands_out and unique describe Product Differentiation\n\nEven if the data and your theory suggest otherwise, explore what happens when you include more or fewer factors in your EFA.\n\npsych::fa(brand_rep, nfactors = 2)$loadings\n\n\nLoadings:\n               MR1    MR2   \nwell_made              0.884\nconsistent             0.932\npoor_workman_r         0.728\nhigher_price    0.745       \nlot_more        0.784 -0.131\ngo_up           0.855 -0.103\nstands_out      0.591  0.286\nunique          0.540  0.307\none_of_a_kind   0.378  0.305\n\n                 MR1   MR2\nSS loadings    2.688 2.485\nProportion Var 0.299 0.276\nCumulative Var 0.299 0.575\n\npsych::fa(brand_rep, nfactors = 4)$loadings\n\n\nLoadings:\n               MR2    MR1    MR3    MR4   \nwell_made       0.879                     \nconsistent      0.964                     \npoor_workman_r  0.732                     \nhigher_price           0.552  0.150  0.168\nlot_more               0.835              \ngo_up                  0.929              \nstands_out                    0.976       \nunique                        0.932       \none_of_a_kind                        0.999\n\n                 MR2   MR1   MR3   MR4\nSS loadings    2.244 1.866 1.846 1.029\nProportion Var 0.249 0.207 0.205 0.114\nCumulative Var 0.249 0.457 0.662 0.776\n\n\nThe two-factor loading worked okay. The 4 factor loading only loaded one variable to the fourth factor. In this example the SME expected a three-factor model and the data did not contradict the theory, so stick with three.\nWhereas the item generation phase tested for item equivalence, the EFA phase tests for internal reliability (consistency) of items. Internal reliability means the survey produces consistent results. The more common statistics for assessing internal reliability are Cronbach’s Alpha, and split-half.\n\n\n2.3.3 Cronbach’s Alpha\nIn general, an alpha &lt;.6 is unacceptable, &lt;.65 is undesirable, &lt;.7 is minimally acceptable, &lt;.8 is respectable, &lt;.9 is very good, and &gt;=.9 suggests items are too alike. A very low alpha means items may not be measuring the same construct, so you should drop items. A very high alpha means items are multicollinear, and you should drop items. Here is Cronbach’s alpha for the brand reputation survey, after removing the poorly-loading one_of_a_kind variable.\n\npsych::alpha(brand_rep[, 1:8])$total$std.alpha\n\n[1] 0.8557356\n\n\nThis value is in the “very good” range. Cronbach’s alpha is often used to measure the reliability of a single dimension. Here are the values for the 3 dimensions.\n\npsych::alpha(brand_rep[, 1:3])$total$std # Product Quality\n## [1] 0.8918025\npsych::alpha(brand_rep[, 4:6])$total$std # Willingness to Pay\n## [1] 0.8517566\npsych::alpha(brand_rep[, 7:8])$total$std # Product Differentiation\n## [1] 0.951472\n\nAlpha is &gt;0.7 for each dimension. Sometimes the alpha for our survey as a whole is greater than that of the dimensions. This can happen because Cronbach’s alpha is sensitive to the number of items. Over-inflation of the alpha statistic can be a concern when working with surveys containing a large number of items.\n\n\n2.3.4 Split-Half\nUse psych::splitHalf() to split the survey in half and test whether all parts of the survey contribute equally to measurement. This method is much less popular than Cronbach’s alpha.\n\npsych::splitHalf(brand_rep[, 1:8])\n\nSplit half reliabilities  \nCall: psych::splitHalf(r = brand_rep[, 1:8])\n\nMaximum split half reliability (lambda 4) =  0.93\nGuttman lambda 6                          =  0.92\nAverage split half reliability            =  0.86\nGuttman lambda 3 (alpha)                  =  0.86\nGuttman lambda 2                          =  0.87\nMinimum split half reliability  (beta)    =  0.66\nAverage interitem r =  0.43  with median =  0.4",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#confirmatoryfactoranalysis",
    "href": "02-item_selection.html#confirmatoryfactoranalysis",
    "title": "2  Item Selection",
    "section": "2.4 Confirmatory Factor Analysis",
    "text": "2.4 Confirmatory Factor Analysis\nWhereas EFA is used to develop a theory of the number of factors needed to explain the relationships among the survey items, confirmatory factor analysis (CFA) is a formal hypothesis test of the EFA theory. CFA measures construct validity, that is, whether you are really measuring what you claim to measure.\nThese notes explain how to use CFA, but do not explain the theory. For that you need to learn about dimensionality reduction, and structural equation modeling.\nUse the lavaan package (latent variable analysis package), passing in the model definition. Here is the model for the three dimensions in the brand reputation survey. Lavaan’s default estimator is maximum likelihood, which assumes normality. You can change it to MLR which uses robust standard errors to mitigate non-normality. The summary prints a ton of output. Concentrate on the lambda - the factor loadings.\n\nbrand_rep_mdl &lt;- paste(\n  \"PrdQl =~ well_made + consistent + poor_workman_r\",\n  \"WillPay =~ higher_price + lot_more + go_up\",\n  \"PrdDff =~ stands_out + unique\", \n  sep = \"\\n\"\n)\nbrand_rep_cfa &lt;- lavaan::cfa(model = brand_rep_mdl, data = brand_rep[, 1:8], estimator = \"MLR\")\n# lavaan::summary(brand_rep_cfa, fit.measures = TRUE, standardized = TRUE)\nsemPlot::semPaths(brand_rep_cfa, rotation = 4)\n\n\n\n\n\n\n\nlavaan::inspect(brand_rep_cfa, \"std\")$lambda\n\n               PrdQl WillPy PrdDff\nwell_made      0.914  0.000  0.000\nconsistent     0.932  0.000  0.000\npoor_workman_r 0.730  0.000  0.000\nhigher_price   0.000  0.733  0.000\nlot_more       0.000  0.812  0.000\ngo_up          0.000  0.902  0.000\nstands_out     0.000  0.000  0.976\nunique         0.000  0.000  0.930\n\n\nThe CFA hypothesis test is a chi-square test, so is sensitive to normality assumptions and n-size. Other fit measure are reported too: * Comparative Fit Index (CFI) (look for value &gt;.9) * Tucker-Lewis Index (TLI) (look for value &gt;.9) * Root mean squared Error of Approximation (RMSEA) (look for value &lt;.05)\nThere are actually 78 fit measures to choose from! Focus on CFI and TLI.\n\nlavaan::fitMeasures(brand_rep_cfa, fit.measures = c(\"cfi\", \"tli\"))\n\n  cfi   tli \n0.980 0.967 \n\n\nThis output indicates a good model because both measures are &gt;.9. Check the standardized estimates for each item. The standardized factor loadings are the basis of establishing construct validity. While we call these measures ‘loadings,’ they are better described as correlations of each manifest item with the dimensions. As you calculated, the difference between a perfect correlation and the observed is considered ‘error.’ This relationship between the so-called ‘true’ and ‘observed’ scores is the basis of classical test theory.\n\nlavaan::standardizedSolution(brand_rep_cfa) %&gt;%\n  filter(op == \"=~\") %&gt;%\n  select(lhs, rhs, est.std, pvalue)\n\n      lhs            rhs est.std pvalue\n1   PrdQl      well_made   0.914      0\n2   PrdQl     consistent   0.932      0\n3   PrdQl poor_workman_r   0.730      0\n4 WillPay   higher_price   0.733      0\n5 WillPay       lot_more   0.812      0\n6 WillPay          go_up   0.902      0\n7  PrdDff     stands_out   0.976      0\n8  PrdDff         unique   0.930      0\n\n\nIf you have a survey that meets your assumptions, performs well under EFA, but fails under CFA, return to your survey and revisit your scale, examine the CFA modification indices, factor variances, etc.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#convergentvalidity",
    "href": "02-item_selection.html#convergentvalidity",
    "title": "2  Item Selection",
    "section": "2.5 Convergent/Discriminant Validity",
    "text": "2.5 Convergent/Discriminant Validity\nConstruct validity means the survey measures what it intends to measure. It is composed of convergent validity and discriminant validity. Convergent validity means factors address the same concept. Discriminant validity means factors address different aspects of the concept.\nTest for construct validity after assessing CFA model strength (with CFI, TFI, and RMSEA) – a poor-fitting model may have greater construct validity than a better-fitting model. Use the semTools::reliability() function. The average variance extracted (AVE) measures convergent validity (avevar) and should be &gt; .5. The composite reliability (CR) measures discriminant validity (omega) and should be &gt; .7.\n\nsemTools::reliability(brand_rep_cfa)\n\n           PrdQl   WillPay    PrdDff\nalpha  0.8926349 0.8521873 0.9514719\nomega  0.9017514 0.8605751 0.9520117\nomega2 0.9017514 0.8605751 0.9520117\nomega3 0.9019287 0.8646975 0.9520114\navevar 0.7573092 0.6756174 0.9084654\n\n\nThese values look good for all three dimensions. As an aside, alpha is Cronbach’s alpha. Do not be tempted to test reliability and validity in the same step. Start with reliability because it is a necessary but insufficient condition for validity. By checking for internal consistency first, as measured by alpha, then construct validity, as measured by AVE and CR, you establish the necessary reliability of the scale as a whole was met, then took it to the next level by checking for construct validity among the unique dimensions.\nAt this point you have established that the latent and manifest variables are related as hypothesized, and that the survey measures what you intended to measure, in this case, brand reputation.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#replication",
    "href": "02-item_selection.html#replication",
    "title": "2  Item Selection",
    "section": "2.6 Replication",
    "text": "2.6 Replication\nThe replication phase establishes criterion validity and stability (reliability). Criterion validity is a measure of the relationship between the construct and some external measure of interest. Measure criterion validity with concurrent validity, how well items correlate with an external metric measured at the same time, and with predictive validity, how well an item predicts an external metric. Stability means the survey produces similar results over repeated test-retest administrations.\n\n2.6.1 Criterion Validity\n\n2.6.1.1 Concurrent Validity\nConcurrent validity is a measure of whether our latent construct is significantly correlated to some outcome measured at the same time.\nSuppose you have an additional data set of consumer spending on the brand. The consumer’s perception of the brand should correlate with their spending. Before checking for concurrent validity, standardize the data so that likert and other variable types are on the same scale.\n\nset.seed(20201004)\nbrand_rep &lt;- brand_rep %&gt;%\n  mutate(spend = ((well_made + consistent + poor_workman_r)/3 * 5 +\n                  (higher_price + lot_more + go_up)/3 * 3 +\n                  (stands_out + unique)/2 * 2) / 10)\nbrand_rep$spend &lt;- brand_rep$spend + rnorm(559, 5, 4) # add randomness\nbrand_rep_scaled &lt;- scale(brand_rep)\n\nDo respondents with higher scores on our the brand reputation scale also tend to spend more at the store? Build model, and latentize spend as Spndng and model with the ~~ operator. Fit the model with the semTools::sem() function.\n\nbrand_rep_cv_mdl &lt;- paste(\n  \"PrdQl =~ well_made + consistent + poor_workman_r\",\n  \"WillPay =~ higher_price + lot_more + go_up\",\n  \"PrdDff =~ stands_out + unique\",\n  \"Spndng =~ spend\",\n  \"Spndng ~~ PrdQl + WillPay + PrdDff\",\n  sep = \"\\n\"\n)\nbrand_rep_cv &lt;- lavaan::sem(data = brand_rep_scaled, model = brand_rep_cv_mdl)\n\nHere are the standardized covariances. Because the data is standardized, interpret these as correlations. The p-vales are not significant because the spending data was random.\n\nlavaan::standardizedSolution(brand_rep_cv) %&gt;% \n  filter(rhs == \"Spndng\") %&gt;%\n  select(-op, -rhs)\n\n      lhs est.std    se     z pvalue ci.lower ci.upper\n1   PrdQl   0.174 0.043 4.092      0    0.091    0.258\n2 WillPay   0.241 0.043 5.654      0    0.157    0.324\n3  PrdDff   0.198 0.041 4.779      0    0.117    0.279\n4  Spndng   1.000 0.000    NA     NA    1.000    1.000\n\nsemPlot::semPaths(brand_rep_cv, whatLabels = \"est.std\", edge.label.cex = .8, rotation = 2)\n\n\n\n\n\n\n\n\nEach dimension of brand reputation is positively correlated to spending history and the relationships are all significant.\n\n\n2.6.1.2 Predictive Validity\nPredictive validity is established by regressing some future outcome on your established construct. Assess predictive validity just as you would with any linear regression – regression estimates and p-values (starndardizedSolution()), and the r-squared coefficient of determination inspect().\nBuild a regression model with the single ~ operator. Then fit the model to the data as before.\n\nbrand_rep_pv_mdl &lt;- paste(\n  \"PrdQl =~ well_made + consistent + poor_workman_r\",\n  \"WillPay =~ higher_price + lot_more + go_up\",\n  \"PrdDff =~ stands_out + unique\",\n  \"spend ~ PrdQl + WillPay + PrdDff\",\n  sep = \"\\n\"\n)\nbrand_rep_pv &lt;- lavaan::sem(data = brand_rep_scaled, model = brand_rep_pv_mdl)\n#lavaan::summary(brand_rep_pv, standardized = T, fit.measures = T, rsquare = T)\nsemPlot::semPaths(brand_rep_pv, whatLabels = \"est.std\", edge.label.cex = .8, rotation = 2)\n\n\n\n\n\n\n\nlavaan::standardizedSolution(brand_rep_pv) %&gt;% \n  filter(op == \"~\") %&gt;%\n  mutate_if(is.numeric, round, digits = 3)\n\n    lhs op     rhs est.std    se     z pvalue ci.lower ci.upper\n1 spend  ~   PrdQl   0.094 0.048 1.949  0.051   -0.001    0.189\n2 spend  ~ WillPay   0.182 0.052 3.486  0.000    0.080    0.284\n3 spend  ~  PrdDff   0.060 0.055 1.092  0.275   -0.047    0.167\n\nlavaan::inspect(brand_rep_pv, \"r2\")\n\n     well_made     consistent poor_workman_r   higher_price       lot_more \n         0.837          0.867          0.533          0.537          0.658 \n         go_up     stands_out         unique          spend \n         0.816          0.951          0.866          0.072 \n\n\nThere is a statistically significant relationship between one dimension of brand quality (Willingness to Pay) and spending. At this point you may want to drop the other two dimensions. However, the R^2 is not good - only 7% of the variability in Spending can be explained by the three dimension of our construct.\nFactor scores represent individual respondents’ standing on a latent factor. While not used for scale validation per se, factor scores can be used for customer segmentation via clustering, network analysis and other statistical techniques.\n\nbrand_rep_cfa &lt;- lavaan::cfa(brand_rep_pv_mdl, data = brand_rep_scaled)\n\nbrand_rep_cfa_scores &lt;- lavaan::predict(brand_rep_cfa) %&gt;% as.data.frame()\npsych::describe(brand_rep_cfa_scores)\n\n        vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis\nPrdQl      1 559    0 0.88  -0.50   -0.19 0.09 -0.57 4.02  4.59  2.31     6.18\nWillPay    2 559    0 0.69   0.00    0.01 0.86 -1.15 1.16  2.31 -0.05    -1.18\nPrdDff     3 559    0 0.96  -0.04   -0.15 1.17 -0.88 2.54  3.41  1.09     0.35\n          se\nPrdQl   0.04\nWillPay 0.03\nPrdDff  0.04\n\npsych::multi.hist(brand_rep_cfa_scores)\n\n\n\n\n\n\n\nmap(brand_rep_cfa_scores, shapiro.test)\n\n$PrdQl\n\n    Shapiro-Wilk normality test\n\ndata:  .x[[i]]\nW = 0.66986, p-value &lt; 2.2e-16\n\n\n$WillPay\n\n    Shapiro-Wilk normality test\n\ndata:  .x[[i]]\nW = 0.94986, p-value = 7.811e-13\n\n\n$PrdDff\n\n    Shapiro-Wilk normality test\n\ndata:  .x[[i]]\nW = 0.82818, p-value &lt; 2.2e-16\n\n\nThese scores are not normally distributed, which makes clustering a great choice for modeling factor scores. Clustering does not mean distance-based clustering, such as K-means, in this context. Mixture models consider data as coming from a distribution which itself is a mixture of clusters. To learn more about model-based clustering in the Hierarchical and Mixed Effects Models DataCamp course.\nFactor scores can be extracted from a structural equation model and used as inputs in other models. For example, you can use the factor scores from the brand reputation dimensions as regressors for a regrssion on spending.\n\nbrand_rep_fs_reg_dat &lt;- bind_cols(brand_rep_cfa_scores, spend = brand_rep$spend)\nbrand_rep_fs_reg &lt;- lm(spend ~ PrdQl + WillPay + PrdDff, data = brand_rep_fs_reg_dat)\nsummary(brand_rep_fs_reg)$coef\n\n             Estimate Std. Error    t value      Pr(&gt;|t|)\n(Intercept) 7.1555354  0.1591195 44.9695738 3.363705e-187\nPrdQl       0.4260875  0.2062002  2.0663774  3.925620e-02\nWillPay     1.1365087  0.2805960  4.0503388  5.842799e-05\nPrdDff      0.1714031  0.2181813  0.7855993  4.324375e-01\n\n\nThe coefficients and r-squared of the lm() and sem() models closely resemble each other, but keeping the regression inside the lavaan framework provides more information (as witnessed in the higher estimates and r-squared). A construct, once validated, can be combined with a wide range of outcomes and models to produce valuable information about consumer behavior and habits.\n\n\n\n2.6.2 Test-Retest Reliability\nTest-retest reliability is the ability to achieve the same result from a respondent at two closely-spaced points in time (repeated measures).\nSuppose you had two surveys, identified by an id field.\n\n# svy_1 &lt;- brand_rep[sample(1:559, 300),] %&gt;% as.data.frame()\n# svy_2 &lt;- brand_rep[sample(1:559, 300),] %&gt;% as.data.frame()\n# survey_test_retest &lt;- psych::testRetest(t1 = svy_1, t2 = svy_2, id = \"id\")\n# survey_test_retest$r12\n\nAn r^2 &lt;.7 is unacceptable, &lt;.9 good, and &gt;.9 very good. This one is unacceptable.\nOne way to check for replication is by splitting the data in half.\n\n# svy &lt;- bind_rows(svy_1, svy_2, .id = \"time\")\n# \n# psych::describeBy(svy, \"time\")\n# \n# brand_rep_test_retest &lt;- psych::testRetest(\n#   t1 = filter(svy, time == 1),\n#   t2 = filter(svy, time == 2),\n#   id = \"id\")\n# \n# brand_rep_test_retest$r12\n\nIf the correlation of scaled scores across time 1 and time 2 is greater than .9, that indicates very strong test-retest reliability. This measure can be difficult to collect because it requires the same respondents to answer the survey at two points in time. However, it’s a good technique to have in your survey development toolkit.\nWhen validating a scale, it’s a good idea to split the survey results into two samples, using one for EFA and one for CFA. This works as a sort of cross-validation such that the overall fit of the model is less likely due to chance of any one sample’s makeup.\n\n# brand_rep_efa_data &lt;- brand_rep[1:280,]\n# brand_rep_cfa_data &lt;- brand_rep[281:559,]\n#  \n# efa &lt;- psych::fa(brand_rep_efa_data, nfactors = 3)\n# efa$loadings\n# \n# brand_rep_cfa &lt;- lavaan::cfa(brand_rep_mdl, data = brand_rep_cfa_data)\n# lavaan::inspect(brand_rep_cfa, what = \"call\")\n# \n# lavaan::fitmeasures(brand_rep_cfa)[c(\"cfi\",\"tli\",\"rmsea\")]\n\n\n\n\n\n\n\nMiddleton, Fiona. 2022. “Reliability Vs. Validity in Research | Difference, Types and Examples.” https://www.scribbr.com/methodology/reliability-vs-validity/.\n\n\nMount, George. n.d. “Survey and Measurement Development in r.” https://app.datacamp.com/learn/courses/survey-and-measurement-development-in-r.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "02-item_selection.html#footnotes",
    "href": "02-item_selection.html#footnotes",
    "title": "2  Item Selection",
    "section": "",
    "text": "This section is aided by Fiona Middleton’s “Reliability vs. Validity in Research | Difference, Types and Examples” (Middleton 2022).↩︎\nThis section is primarily from George Mount’s Data Camp course (Mount, n.d.).↩︎",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Item Selection</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html",
    "href": "03-sampling_strategy.html",
    "title": "3  Sampling Strategy",
    "section": "",
    "text": "3.1 Simple Random Sampling\nSimple Random Sampling (SRS) designs are almost never used because they",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html#sec-sd-srs",
    "href": "03-sampling_strategy.html#sec-sd-srs",
    "title": "3  Sampling Strategy",
    "section": "",
    "text": "3.1.1 For Continuous Values\nSuppose a survey variable estimate a population (universal) mean, \\(\\bar{Y}\\), from the mean of a simple random sample, \\(\\bar{y}\\). Under repeated sampling, the sample means would have a variance related to the population variance, \\(S^2\\).\n\\[\nV(\\bar{y}) = \\left(1 - \\frac{n}{N} \\right) \\frac{S^2}{n}\n\\tag{3.1}\\]\nEquation 3.1 is the square of the standard error with a finite population correction factor (FPC), \\(1 - n / N\\). The FPC reduces the expected variance for small populations. In practice, the FPC is only important when \\(n/N &lt; .05\\). \\(S\\) is an unknown population parameter estimated from the sample, \\(s\\), or by a rule of thumb.\nThe ratio \\(V(\\bar{y}) / \\bar{Y}^2\\) is the square of the targeted coefficient of variation, \\(CV_0\\).\n\\[\nCV_0^2(\\bar{Y}) = \\left(1 - \\frac{n}{N} \\right) \\cdot \\frac{1}{n} \\cdot \\frac{S^2}{\\bar{Y}^2}\n\\tag{3.2}\\]\nSolve Equation 3.2 for the required sample size to achieve \\(CV_0(\\bar{Y})\\).\n\\[\nn = \\frac{S^2 / \\bar{Y}^2}{CV_0^2 + S^2 / (N \\cdot \\bar{Y}^2)}\n\\tag{3.3}\\]\nThe numerator in Equation 3.3 is the population CV (a.k.a., unit CV).\nUse PracTools::nCont() to calculate \\(n\\). Setting the unit CV is somewhat of a chicken-and-egg problem since \\(S^2\\) and \\(\\bar{Y}^2\\) are the population parameters you are estimating. Either rely on prior research or a best guess. The range rule of thumb is \\(S = \\mathrm{range} / 4\\). The targeted CV is usually set to 5% or 10%, or something better than prior research.\nExample. Prior experience suggests the unit CV is approximately \\(2\\). Your survey targets \\(CV_0(\\bar{y}_s) = 0.05\\).\nN defaults to Inf, fine for “large” populations.\n\nPracTools::nCont(CV0 = .05, CVpop = 2)\n\n[1] 1600\n\n\nFor a company survey, include N. for 10,000 employees you can survey a few less.\n\nPracTools::nCont(CV0 = .05, CVpop = 2, N = 10000)\n\n[1] 1379.31\n\n\nEven for a small population, you can still survey about half.\n\nPracTools::nCont(CV0 = .05, CVpop = 2, N = 1000)\n\n[1] 615.3846\n\n\nIf you don’t know CVpop or ybarU and S2, but have an expectation about ybarU and the range, use the range rule of thumb.\n\nPracTools::nCont(CV0 = .10, S2 = (abs(0 - 800) / 4)^2, ybarU = 100)\n\n[1] 400\n\n\nWhen does N become important? It depends on CV0, but N=20,000 seems to be upper limit.\n\n\n\n\nRequired sample size given population and CV requirements.\n\n\n\n\n\nAlternatively, you can target a margin of error.\n\\[\nMOE = t_{(1-\\alpha), (n-1)} SE(\\bar{y})\n\\tag{3.4}\\]\nExample. You want a margin of error of 20. You estimate an overall range of 800.\n\nPracTools::nContMoe(moe.sw = 1, e = 20, S2 = (abs(0 - 800) / 4)^2) |&gt; ceiling()\n\n[1] 385\n\n\n\n\n3.1.2 For Proportions\nIf the population parameter is a proportion, \\(p\\), the CV is\n\\[\nCV^2(p_s) = \\left(1 - \\frac{n}{N} \\right) \\cdot \\frac{1}{n} \\cdot \\frac{N}{N-1} \\cdot \\frac{1 - p_U}{p_U}\n\\tag{3.5}\\]\nwhere \\(\\frac{N}{N-1} \\cdot \\frac{1 - p_U}{p_U}\\) is the square of the unit CV. When \\(N\\) is large, Equation 3.5 reduces to \\(CV^2(p_s) \\approx \\frac{1}{n} \\cdot \\frac{1 - p_U}{P_U}\\). From here you can see that \\(n\\) varies inversely with \\(p_U\\). PracTools::nProp() calculates \\(n\\) for proportions.\nExample. From prior experience you think \\(p_U = .01\\) and \\(N\\) is large. You set a targeted CV of \\(CV_0^2(p_s) = 0.05\\).\n\nPracTools::nProp(CV0 = .05, pU = .01)\n\n[1] 39600\n\n\nWhoa, \\(n\\) is huge!\nYou might choose to target a margin of error instead, \\(MOE = \\pm z_{1-\\alpha} \\cdot SE\\). Recall that \\(P(|p_s - p_U| &lt; MOE) = 1 - \\alpha\\). PracTools::nPropMoe() and PracTools::nContMoe() calculate \\(n\\) for MOEs.\nExample. Continuing from above, suppose you set a tolerance of a half percentage point, \\(MOE \\pm 0.5\\%\\) for an expected proportion of around 1%.\n\n# moe.sw = 1 sets MOE based on SE; moe.sw = 2 sets MOE based on CV.\nPracTools::nPropMoe(moe.sw = 1, e = .005, alpha = .05, pU = .01)\n\n[1] 1521.218\n\n\nOr you can use nProp() specifying the targeted variance of the esimated propotion (v0) with an estimate of the population proportion.\n\nz_025 &lt;- qnorm(p = .05/2, lower.tail = FALSE)\n\nSE &lt;- .005 / z_025\n\nPracTools::nProp(V0 = SE^2, pU = .01)\n\n[1] 1521.218\n\n\nWhen \\(p_U\\) is extreme (~0 or ~1), the 95% CI can pass the [0,1] limits. The Wilson method accounts for that. Notice the 95% CI is not symmetric about \\(p_U\\). The 95% CI calculation is the main reason it is used.\n\nPracTools::nWilson(moe.sw = 1, e = .005, alpha = .05, pU = .01)\n\n$n.sam\n[1] 1605.443\n\n$`CI lower limit`\n[1] 0.00616966\n\n$`CI upper limit`\n[1] 0.01616966\n\n$`length of CI`\n[1] 0.01\n\n\nThe log odds is another approach that does about the same thing, but no 95% CI.\n\nPracTools::nLogOdds(moe.sw = 1, e = .005, alpha = .05, pU = .01)\n\n[1] 1637.399",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html#sec-stratified-srs",
    "href": "03-sampling_strategy.html#sec-stratified-srs",
    "title": "3  Sampling Strategy",
    "section": "3.2 Stratified SRS",
    "text": "3.2 Stratified SRS\nStratified samples partition the population by dimensions of interest before sampling. This way, important domains are assured of adequate representation. Stratifying often reduce variances. Choose stratification if i) an SRS risks poor distribution across the population, ii) you have domains you will study separately, or iii) there are units with similar mean and variances that can be grouped to increase efficiency.\nIn a stratified design, the measured mean or proportion of the population is the simple weighted sum of the \\(h\\) strata, \\(\\bar{y}_{st} = \\sum{W_h}\\bar{y}_{sh}\\) and \\(p_{st} = \\sum{W_h}p_{sh}\\). The population sampling variance is analogous,\n\\[\nV(\\bar{y}_{st}) = \\sum W_h^2 \\cdot \\left(1 - \\frac{n_h}{N} \\right) \\cdot \\frac{1}{n_h} \\cdot S_h^2.\n\\tag{3.6}\\]\nUse the SRS sampling methods described in Section Section 3.1 to estimate each stratum.\nThe effect of stratification relative to SRS is captured in the design effect ratio,\n\\[\nD^2(\\hat{\\theta}) = \\frac{V(\\hat{\\theta})_\\mathrm{complex}}{V(\\hat{\\theta})_\\mathrm{SRS}}\n\\]\nExample. Suppose you are measuring expenditure in a company of \\(N = 875\\) employees and want to stratify by the \\(h = 6\\) departments, with target \\(CV_0(\\bar{y_s}) = .10.\\)\n\ndata(smho98, package = \"PracTools\")\n\nsmho98 |&gt;\n  summarize(.by = STRATUM, Nh = n(), Mh = mean(EXPTOTAL), Sh = sd(EXPTOTAL)) |&gt;\n  mutate(\n    CVpop = Sh / Mh,\n    nh = ceiling(map2_dbl(CVpop, Nh, ~PracTools::nCont(CV0 = .10, CVpop = .x, N = .y)))\n  ) |&gt;\n  janitor::adorn_totals(\"row\", fill = NULL, na.rm = FALSE, name = \"Total\", Nh, nh) |&gt;\n  knitr::kable()\n\n\n\n\nSTRATUM\nNh\nMh\nSh\nCVpop\nnh\n\n\n\n\n1\n151\n13066021.0\n14792181.7\n1.1321107\n70\n\n\n2\n64\n40526852.3\n37126887.6\n0.9161059\n37\n\n\n3\n43\n11206761.2\n11531709.1\n1.0289957\n31\n\n\n4\n22\n7714828.7\n8422580.5\n1.0917391\n19\n\n\n5\n150\n4504517.9\n6228212.0\n1.3826589\n85\n\n\n6\n23\n2938983.2\n3530750.8\n1.2013511\n20\n\n\n7\n65\n7207527.1\n9097011.5\n1.2621543\n47\n\n\n8\n14\n1879611.1\n1912347.8\n1.0174168\n13\n\n\n9\n38\n13841123.7\n11609453.3\n0.8387652\n25\n\n\n10\n12\n5867993.8\n6427340.3\n1.0953216\n11\n\n\n11\n13\n925512.8\n659352.8\n0.7124189\n11\n\n\n12\n77\n4740554.8\n6905571.9\n1.4567012\n57\n\n\n13\n59\n9060838.1\n12884439.0\n1.4219920\n46\n\n\n14\n86\n9511349.0\n6741831.0\n0.7088196\n32\n\n\n15\n39\n34251923.7\n82591916.4\n2.4113074\n37\n\n\n16\n19\n4629061.5\n9817393.7\n2.1208173\n19\n\n\nTotal\n875\nNA\nNA\nNA\n560\n\n\n\n\n\nWith SRS, the required sample is is only 290.\n\nsmho98 %&gt;%\n  summarize(Nh = n(), Mh = mean(EXPTOTAL), Sh = sd(EXPTOTAL)) |&gt;\n  mutate(\n    CVpop = Sh / Mh,\n    nh = ceiling(map2_dbl(CVpop, Nh, ~PracTools::nCont(CV0 = .10, CVpop = .x, N = .y)))\n  ) |&gt;\n  knitr::kable()\n\n\n\n\nNh\nMh\nSh\nCVpop\nnh\n\n\n\n\n875\n11664181\n24276522\n2.081288\n290\n\n\n\n\n\nIf a fixed budget constrains you to \\(n\\) participants you have five options: i) if \\(S_h\\) are approximately equal and you are okay with small stratum getting very few units, allocate \\(n\\) by proportion, \\(n_h = nW_h\\); ii) if your strata are study domains, allocate \\(n\\) equally, \\(n_h = n / H\\); iii) use Neyman allocation to minimize the population sampling variance; iv) use cost-constrained allocation to minimize cost, or v) use precision-constrained allocation to minimize population sampling variance. Options iv and v take into account variable costs. Use function PracTools::strAlloc().\nThe Neyman allocation allocates by stratum weight.\n\\[\nn_h = n \\cdot \\frac{W_h S_h}{\\sum W_h S_h}\n\\]\nSuppose costs vary by stratum, \\(c_h\\). The cost-constrained allocation allocates more population to larger strata and strata with larger variances. Starting with \\(C = c_0 + \\sum n_h c_h\\), minimize the population sampling variance.\n\\[\nn_h = (C - c_0) \\frac{W_hS_h / \\sqrt{c_h}}{\\sum W_h S_h \\sqrt{c_h}}\n\\]\nThe precision-constrained allocation is\n\\[\nn_h = (W_h S_h / \\sqrt{c_h}) \\frac{\\sum W_h S_h \\sqrt{c_h}}{V_0 + N^{-1} \\sum W_h S_h^2}.\n\\]\nExample. Suppose you have a fixed budget of $100,000. If sampling costs are $1,000 person, survey \\(n = 100\\) people and allocate \\(n\\) to \\(n_h\\) with options i-iii). If sampling costs vary by stratum, use options iv-v).\n\n# Stratum per capita survey costs\nch &lt;- c(1400, 400, 300, 600, 450, 1000, 950, 250, 350, 650, 450, 950, 80, 70, 900, 80)\n\nsmho98 |&gt;\n  summarize(.by = STRATUM, Nh = n(), Mh = mean(EXPTOTAL), Sh = sd(EXPTOTAL)) %&gt;%\n  bind_cols(\n    `i) prop` = ceiling(.$Nh / sum(.$Nh) * 100),\n    `ii) equal` = ceiling(1 / nrow(.) * 100),\n    `iii) neyman` = ceiling(PracTools::strAlloc(\n      n.tot = 100, Nh = .$Nh, Sh = .$Sh, alloc = \"neyman\"\n    )$nh),\n    ch = ch,\n    `iv) cost` = ceiling(PracTools::strAlloc(\n      Nh = .$Nh, Sh = .$Sh, cost = 100000, ch = ch, alloc = \"totcost\"\n    )$nh),\n    `v) prec.` = ceiling(PracTools::strAlloc(\n      Nh = .$Nh, Sh = .$Sh, CV0 = .10, ch = ch, ybarU = .$Mh, alloc = \"totvar\"\n    )$nh)\n  ) |&gt;\n  select(-c(Mh, Sh)) |&gt;\n  janitor::adorn_totals() |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTRATUM\nNh\ni) prop\nii) equal\niii) neyman\nch\niv) cost\nv) prec.\n\n\n\n\n1\n151\n18\n7\n18\n1400\n19\n12\n\n\n2\n64\n8\n7\n19\n400\n37\n3\n\n\n3\n43\n5\n7\n4\n300\n9\n7\n\n\n4\n22\n3\n7\n2\n600\n3\n3\n\n\n5\n150\n18\n7\n8\n450\n14\n25\n\n\n6\n23\n3\n7\n1\n1000\n1\n2\n\n\n7\n65\n8\n7\n5\n950\n6\n8\n\n\n8\n14\n2\n7\n1\n250\n1\n2\n\n\n9\n38\n5\n7\n4\n350\n8\n5\n\n\n10\n12\n2\n7\n1\n650\n1\n2\n\n\n11\n13\n2\n7\n1\n450\n1\n1\n\n\n12\n77\n9\n7\n5\n950\n6\n10\n\n\n13\n59\n7\n7\n6\n80\n27\n26\n\n\n14\n86\n10\n7\n5\n70\n22\n20\n\n\n15\n39\n5\n7\n26\n900\n34\n4\n\n\n16\n19\n3\n7\n2\n80\n7\n12\n\n\nTotal\n875\n108\n112\n108\n8880\n196\n142",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html#sec-power-analysis",
    "href": "03-sampling_strategy.html#sec-power-analysis",
    "title": "3  Sampling Strategy",
    "section": "3.3 Power Analysis",
    "text": "3.3 Power Analysis\nSection 3.1 and Section 3.2 calculated sample sizes based on the desired precision of the population parameter using CV, MOE, and cost constraints. Another approach is to calculate the sample size required to detect the alternative value in a hypothesis test. Power is a measure of the likelihood of detecting some magnitude difference \\(\\delta\\) between \\(H_0\\) and \\(H_a\\).2 Power calculations are best suited for studies that estimate theoretical population values, not for studies that estimate group differences in a finite population (Valliant 2013).\nA measured \\(t = \\frac{\\hat{\\bar{y}} - \\mu_0}{\\sqrt{v(\\hat{\\bar{y}})}}\\) test statistic would vary with repeated measurements and have a \\(t\\) distribution. A complication about the degrees of freedom arises in survey analysis. It is usually defined using a rule of thumb: \\(df = n_{psu} - n_{strata}\\). So if you have 10 strata and 100 PSUs per stratum, \\(df\\) would equal 1,000 - 100 = 900.\nExample. Suppose you want to measure mean household income for married couples. From prior research, you expect the mean is $55,000 with 6% CV. You hypothesize \\(\\mu\\) is greater than $55,000, but only care if the difference is at least $5,000.\nThe 6% CV implies SE = 6% * $55,000 = $3,300. Supposing \\(\\sigma\\) = $74,000, the original research would have use a sample of size n = \\((\\$74,000 / \\$3,300)^2\\) = 503.\nDon’t use n = 503 for your sample though. The probability of measuring a sample mean &gt;= $60,000 with an acceptable p-value is the power of the study. For n = 503 the power is only 0.448. The area of 1 - \\(\\beta\\) in the top panel below is only pnorm(qnorm(.95, 50000, 3300), 55000, 3300, lower.tail = FALSE) = 0.448. To achieve a 1-\\(\\beta\\) = .80 power, you need n = 1,356. That’s what the bottom panel shows. Notice that a sample mean of $59,000 still rejects H0: \\(\\mu\\) = $55,000. The power of the test tells you the sample size you need to draw a sample mean large enough to reject H0 1-\\(\\beta\\) percent of the time.\n\n\n\n\n\n\n\n\n\nThe power of the test from the original study was only 0.448.\n\npower.t.test(\n  type = \"one.sample\",\n  n = 503, \n  delta = 5000, \n  sd = 74000, \n  sig.level = .05, \n  alternative = \"one.sided\"\n)\n\n\n     One-sample t test power calculation \n\n              n = 503\n          delta = 5000\n             sd = 74000\n      sig.level = 0.05\n          power = 0.4476846\n    alternative = one.sided\n\n\nWith such a low power of the study, a sample mean of $59,000 isn’t large enough to reject H0. Its p-value would be pt(q = (59000-55000)/(74000/sqrt(503)), df = 503 - 1, lower.tail = FALSE) = 0.113. To find the right sample size, use the power calculation with 1 - \\(\\beta\\) and n unspecified.\n\npower.t.test(\n  type = \"one.sample\",\n  delta = 5000, \n  sd = 74000,\n  sig.level = .05,\n  power = .80,\n  alternative = \"one.sided\"\n)\n\n\n     One-sample t test power calculation \n\n              n = 1355.581\n          delta = 5000\n             sd = 74000\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html#appendix-bias",
    "href": "03-sampling_strategy.html#appendix-bias",
    "title": "3  Sampling Strategy",
    "section": "3.4 Appendix: Bias",
    "text": "3.4 Appendix: Bias\nA consideration not explored here, but which should be on your mind is the risk of bias. Here are a few types of bias to beware of.\n\nCoverage bias. The sampling frame is not representative of the population. E.g., school club members is a poor sampling frame if target population is high school students.\nSampling bias. The sample itself is not representative of the population. This occurs when response rates differ, or sub-population sizes differ. Explicitly define the target population and sampling frame, and use systematic sampling methods such as stratified sampling. Adjust analysis and interpretation for response rate differences.\nNon-response bias. Responded have different attributes than non-respondents. You can offer incentives to increase response rate, follow up with non-respondents to find out the reasons for their lack of response, or compare the characteristics of non-respondents with respondents or known external benchmarks for differences.\nMeasurement bias. Survey results differ from the population values. The major cause is deficient instrument design due to ambiguous items, unclear instructions, or poor usability. Reduce measurement bias with pretesting or pilot testing of the instrument, and formal tests for validity and reliability.\n\n\n\n\n\n\n\nValliant, Jill A.; Kreuter, Richard; Dever. 2013. Practical Tools for Designing and Weighting Survey Samples. Springer. https://doi.org/10.1007/978-3-319-93632-1.",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "03-sampling_strategy.html#footnotes",
    "href": "03-sampling_strategy.html#footnotes",
    "title": "3  Sampling Strategy",
    "section": "",
    "text": "Cluster sampling, systematic sampling and Poisson sampling are other sampling methods to at least be aware of. I’m not ready to deal with these yet.↩︎\nSee statistics handbook section on frequentist statistics for discussion of Type I and II errors.↩︎",
    "crumbs": [
      "Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Strategy</span>"
    ]
  },
  {
    "objectID": "04-data_preparation.html",
    "href": "04-data_preparation.html",
    "title": "4  Data Preparation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(janitor)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(gtsummary)\n\nThis book uses the api datasets from the survey package for examples. The Academic Performance Index (API) was a school rating system used in California for several years. The survey package includes several datasets that mimic survey samples from the overall population of 6,194 schools.\ndata(api) returns several data files: a simple random sample (apisrs), a stratified simple random sample (apistrat), and a two-stage cluster (apiclus2).\n\ndata(api, package = \"survey\")\n\n# Add some cols for stat test examples.\nprep_data &lt;- function(df) {\n  df |&gt;\n    mutate(\n      stype = factor(stype, levels = c(\"E\", \"M\", \"H\"), ordered = TRUE), \n      meals_cut = cut(meals, c(0, 12, 25, 100), include.lowest = TRUE),\n      hsg_cut = cut(hsg, c(0, 12, 25, 100), include.lowest = TRUE)\n    )\n}\n\napisrs &lt;- prep_data(apisrs)\n\napistrat &lt;- prep_data(apistrat)\n\napiclus2 &lt;- prep_data(apiclus2)\n\nSchools are uniquely identified by column snum. Schools roll up to districts, dnum. Two other columns contain metadata related to the sampling design.\n\nfpc: finite population correction (FPC). The FPC adjusts the variance calculation (Section 3.1). The FPC is important when the sample size is &gt;=5% of the population size. fpc equals the size of the population that the respondent is drawn from. For an SRS, that’s the entire population of 6,194 schools. For a 2 stage cluster design, that’s the second stage population.\npw: sampling weight. The sampling weight scales the sample up to the population. Think of it as saying, “this respondent represents pw respondents from the total population.”\n\nLet’s create the design objects.\n\nSimple Random SampleStratifiedTwo-Stage Cluster\n\n\napisrs is a simple random sample of 200 schools from a population of 6,194, so fpc = 6194 and pw = 6194 / 200 = 30.97 for all rows.\n\napisrs_des &lt;- as_survey_design(apisrs, weights = pw, fpc = fpc)\n\nsummary(apisrs_des)\n\nIndependent Sampling design\nCalled via srvyr\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03229 0.03229 0.03229 0.03229 0.03229 0.03229 \nPopulation size (PSUs): 6194 \nData variables:\n [1] \"cds\"       \"stype\"     \"name\"      \"sname\"     \"snum\"      \"dname\"    \n [7] \"dnum\"      \"cname\"     \"cnum\"      \"flag\"      \"pcttest\"   \"api00\"    \n[13] \"api99\"     \"target\"    \"growth\"    \"sch.wide\"  \"comp.imp\"  \"both\"     \n[19] \"awards\"    \"meals\"     \"ell\"       \"yr.rnd\"    \"mobility\"  \"acs.k3\"   \n[25] \"acs.46\"    \"acs.core\"  \"pct.resp\"  \"not.hsg\"   \"hsg\"       \"some.col\" \n[31] \"col.grad\"  \"grad.sch\"  \"avg.ed\"    \"full\"      \"emer\"      \"enroll\"   \n[37] \"api.stu\"   \"pw\"        \"fpc\"       \"meals_cut\" \"hsg_cut\"  \n\n\n\n\napistrat is a sample of 200 schools from a population stratified by school type, stype: E = Elementary (n = 100, fpc = 4421, pw = 44.2), M = Middle (n = 50, fpc = 1018, pw = 20.4), and H = High School (n = 50, fpc = 755, pw = 15.1). pw equals the fpc / n.\n\napistrat_des &lt;- as_survey_design(apistrat, weights = pw, fpc = fpc, strata = stype)\n\nsummary(apistrat_des)\n\nStratified Independent Sampling design\nCalled via srvyr\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.02262 0.02262 0.03587 0.04014 0.05339 0.06623 \nStratum Sizes: \n             E  H  M\nobs        100 50 50\ndesign.PSU 100 50 50\nactual.PSU 100 50 50\nPopulation stratum sizes (PSUs): \n   E    H    M \n4421  755 1018 \nData variables:\n [1] \"cds\"       \"stype\"     \"name\"      \"sname\"     \"snum\"      \"dname\"    \n [7] \"dnum\"      \"cname\"     \"cnum\"      \"flag\"      \"pcttest\"   \"api00\"    \n[13] \"api99\"     \"target\"    \"growth\"    \"sch.wide\"  \"comp.imp\"  \"both\"     \n[19] \"awards\"    \"meals\"     \"ell\"       \"yr.rnd\"    \"mobility\"  \"acs.k3\"   \n[25] \"acs.46\"    \"acs.core\"  \"pct.resp\"  \"not.hsg\"   \"hsg\"       \"some.col\" \n[31] \"col.grad\"  \"grad.sch\"  \"avg.ed\"    \"full\"      \"emer\"      \"enroll\"   \n[37] \"api.stu\"   \"pw\"        \"fpc\"       \"meals_cut\" \"hsg_cut\"  \n\n\n\n\napiclus2 is a two-stage cluster sample of 126 schools within districts. The first stage is random sample of 40 of the 757 school districts (dnum). The second stage is a random sample of up to 5 schools (snum) from each district. For cluster designs, the cluster ids are specified in the design object from largest to smallest level.\n\napiclus2_des &lt;- as_survey_design(\n  apiclus2,\n  id = c(dnum, snum),\n  weights = pw,\n  fpc = c(fpc1, fpc2),\n)\n\nsummary(apiclus2_des)\n\n2 - level Cluster Sampling design\nWith (40, 126) clusters.\nCalled via srvyr\nProbabilities:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.003669 0.037743 0.052840 0.042390 0.052840 0.052840 \nPopulation size (PSUs): 757 \nData variables:\n [1] \"cds\"       \"stype\"     \"name\"      \"sname\"     \"snum\"      \"dname\"    \n [7] \"dnum\"      \"cname\"     \"cnum\"      \"flag\"      \"pcttest\"   \"api00\"    \n[13] \"api99\"     \"target\"    \"growth\"    \"sch.wide\"  \"comp.imp\"  \"both\"     \n[19] \"awards\"    \"meals\"     \"ell\"       \"yr.rnd\"    \"mobility\"  \"acs.k3\"   \n[25] \"acs.46\"    \"acs.core\"  \"pct.resp\"  \"not.hsg\"   \"hsg\"       \"some.col\" \n[31] \"col.grad\"  \"grad.sch\"  \"avg.ed\"    \"full\"      \"emer\"      \"enroll\"   \n[37] \"api.stu\"   \"pw\"        \"fpc1\"      \"fpc2\"      \"meals_cut\" \"hsg_cut\"",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "05-descriptive-stats.html",
    "href": "05-descriptive-stats.html",
    "title": "5  Descriptive Statistics",
    "section": "",
    "text": "5.1 Survey Summary\napisrs_des |&gt;\n  tbl_svysummary(\n    by  = sch.wide, \n    include = c(api00, target, growth, awards, comp.imp)\n  ) |&gt;\n  modify_spanning_header(all_stat_cols() ~ \"Met Growth Target\") |&gt;\n  as_gt() |&gt;\n  gt::tab_header(\n    title = \"Simple Random Sample Survey Summary\"\n  ) |&gt;\n  gt::tab_options(heading.align = \"left\")\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Random Sample Survey Summary\n\n\nCharacteristic\n\nMet Growth Target\n\n\n\nNo\nN = 1,146\n1\nYes\nN = 5,048\n1\n\n\n\n\napi00\n541 (482, 708)\n675 (577, 772)\n\n\ntarget\n13 (4, 16)\n9 (5, 14)\n\n\n    Unknown\n0\n588\n\n\ngrowth\n-1 (-12, 6)\n33 (20, 51)\n\n\nawards\n0 (0%)\n3,840 (76%)\n\n\ncomp.imp\n62 (5.4%)\n4,057 (80%)\n\n\n\n1\nMedian (Q1, Q3); n (%)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "05-descriptive-stats.html#point-estimates",
    "href": "05-descriptive-stats.html#point-estimates",
    "title": "5  Descriptive Statistics",
    "section": "5.2 Point Estimates",
    "text": "5.2 Point Estimates\nHow many schools met the growth target? survey_count() returns a scaled row count. If you are collecting other statistics, use survey_total(1) to sum the rows. Use the vartype parameter to add cols for interval-related data around the estimate.\n\nSRSStratifiedCluster\n\n\n\napisrs_pe &lt;-\n  apisrs_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Schools = survey_total(1, vartype = NULL),\n    Proportion = survey_mean(proportion = TRUE, vartype = NULL),\n    EnrollSum = survey_total(enroll, vartype = NULL),\n    EnrollMean = survey_mean(enroll, vartype = c(\"se\", \"ci\")),\n    EnrollIQR = survey_quantile(enroll, quantiles = c(.25, .75), vartype = NULL)\n  )\n\n\n\nShow the code\napisrs_pe |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2, 4:10), decimals = 0) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Mean Enrollment\", starts_with(\"EnrollMean\")) |&gt;\n  gt::tab_spanner(\"IQR\", starts_with(\"EnrollIQR\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n      EnrollMean = \"Mean\",\n      EnrollMean_se = \"SE\",\n      EnrollMean_low = \"Low\",\n      EnrollMean_upp = \"Upp\",\n      EnrollIQR_q25 = \"Q25\",\n      EnrollIQR_q75 = \"Q75\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nSchools\nProportion\nEnrollSum\nMean Enrollment\nIQR\n\n\nMean\nSE\nLow\nUpp\nQ25\nQ75\n\n\n\n\nNo\n1,146\n19%\n948,766\n828\n91\n648\n1,008\n412\n1,252\n\n\nYes\n5,048\n81%\n2,672,308\n529\n24\n481\n578\n339\n639\n\n\nNA\n6,194\n100%\n3,621,074\n585\n27\n531\n639\n339\n664\n\n\n\n\n\n\n\n\n\n\napistrat_pe &lt;-\n  apistrat_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Schools = survey_total(1, vartype = NULL),\n    Proportion = survey_mean(proportion = TRUE, vartype = NULL),\n    EnrollSum = survey_total(enroll, vartype = NULL),\n    EnrollMean = survey_mean(enroll, vartype = c(\"se\", \"ci\")),\n    EnrollIQR = survey_quantile(enroll, quantiles = c(.25, .75), vartype = NULL)\n  )\n\n\n\nShow the code\napistrat_pe |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2, 4:10), decimals = 0) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Mean Enrollment\", starts_with(\"EnrollMean\")) |&gt;\n  gt::tab_spanner(\"IQR\", starts_with(\"EnrollIQR\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n      EnrollMean = \"Mean\",\n      EnrollMean_se = \"SE\",\n      EnrollMean_low = \"Low\",\n      EnrollMean_upp = \"Upp\",\n      EnrollIQR_q25 = \"Q25\",\n      EnrollIQR_q75 = \"Q75\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nSchools\nProportion\nEnrollSum\nMean Enrollment\nIQR\n\n\nMean\nSE\nLow\nUpp\nQ25\nQ75\n\n\n\n\nNo\n1,066\n17%\n1,013,067\n951\n95\n764\n1,137\n441\n1,515\n\n\nYes\n5,128\n83%\n2,674,110\n521\n19\n484\n558\n325\n613\n\n\nNA\n6,194\n100%\n3,687,178\n595\n19\n559\n632\n334\n660\n\n\n\n\n\n\n\n\n\n\napiclus2_pe &lt;-\n  apiclus2_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Schools = survey_total(1, vartype = NULL),\n    Proportion = survey_mean(proportion = TRUE, vartype = NULL),\n    EnrollSum = survey_total(enroll, vartype = NULL),\n    EnrollMean = survey_mean(enroll, vartype = c(\"se\", \"ci\")),\n    # EnrollIQR = survey_quantile(enroll, quantiles = c(.25, .75), vartype = NULL)\n  )\n\n\n\nShow the code\napiclus2_pe |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2, 4:8), decimals = 0) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Mean Enrollment\", starts_with(\"EnrollMean\")) |&gt;\n  gt::tab_spanner(\"IQR\", starts_with(\"EnrollIQR\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n    EnrollMean = \"Mean\",\n    EnrollMean_se = \"SE\",\n    EnrollMean_low = \"Low\",\n    EnrollMean_upp = \"Upp\"\n    # EnrollIQR_q25 = \"Q25\",\n    # EnrollIQR_q75 = \"Q75\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nSchools\nProportion\nEnrollSum\nMean Enrollment\n\n\nMean\nSE\nLow\nUpp\n\n\n\n\nNo\n1,276\n25%\n1,104,845\n866\n106\n653\n1,080\n\n\nYes\n3,853\n75%\nNA\nNA\nNA\nNA\nNA\n\n\nNA\n5,129\n100%\nNA\nNA\nNA\nNA\nNA",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "05-descriptive-stats.html#bivariate-relationships",
    "href": "05-descriptive-stats.html#bivariate-relationships",
    "title": "5  Descriptive Statistics",
    "section": "5.3 Bivariate Relationships",
    "text": "5.3 Bivariate Relationships\nBivariate statistics include ratios and correlations.\n\nSRSStratifiedCluster\n\n\n\napisrs_bv &lt;-\n  apisrs_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Meals = survey_total(meals, vartype = NULL),\n    Enrollment = survey_total(enroll, vartype = NULL),\n    RatioEst = survey_ratio(meals, enroll),\n    CorrEst = survey_corr(meals, enroll)\n  )\n\n\n\nShow the code\napisrs_bv |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2:3), decimals = 0) |&gt;\n  gt::fmt_number(columns = c(4:7), decimals = 4) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Ratio\", starts_with(\"Ratio\")) |&gt;\n  gt::tab_spanner(\"Correlation\", starts_with(\"Corr\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n    RatioEst = \"Est\",\n    RatioEst_se = \"SE\",\n    CorrEst = \"Est\",\n    CorrEst_se = \"SE\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nMeals\nEnrollment\nRatio\nCorrelation\n\n\nEst\nSE\nEst\nSE\n\n\n\n\nNo\n58,038\n94,876,595%\n0.0612\n0.0096\n−0.1103\n0.1523\n\n\nYes\n251,724\n267,230,839%\n0.0942\n0.0062\n−0.0360\n0.0560\n\n\nNA\n309,762\n362,107,434%\n0.0855\n0.0055\n−0.0511\n0.0590\n\n\n\n\n\n\n\n\n\n\napistrat_bv &lt;-\n  apisrs_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Meals = survey_total(meals, vartype = NULL),\n    Enrollment = survey_total(enroll, vartype = NULL),\n    RatioEst = survey_ratio(meals, enroll),\n    CorrEst = survey_corr(meals, enroll)\n  )\n\n\n\nShow the code\napistrat_bv |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2:3), decimals = 0) |&gt;\n  gt::fmt_number(columns = c(4:7), decimals = 4) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Ratio\", starts_with(\"Ratio\")) |&gt;\n  gt::tab_spanner(\"Correlation\", starts_with(\"Corr\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n    RatioEst = \"Est\",\n    RatioEst_se = \"SE\",\n    CorrEst = \"Est\",\n    CorrEst_se = \"SE\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nMeals\nEnrollment\nRatio\nCorrelation\n\n\nEst\nSE\nEst\nSE\n\n\n\n\nNo\n58,038\n94,876,595%\n0.0612\n0.0096\n−0.1103\n0.1523\n\n\nYes\n251,724\n267,230,839%\n0.0942\n0.0062\n−0.0360\n0.0560\n\n\nNA\n309,762\n362,107,434%\n0.0855\n0.0055\n−0.0511\n0.0590\n\n\n\n\n\n\n\n\n\n\napiclus2_bv &lt;-\n  apisrs_des |&gt;\n  group_by(sch.wide) |&gt;\n  cascade(\n    Meals = survey_total(meals, vartype = NULL),\n    Enrollment = survey_total(enroll, vartype = NULL),\n    RatioEst = survey_ratio(meals, enroll),\n    CorrEst = survey_corr(meals, enroll)\n  )\n\n\n\nShow the code\napiclus2_bv |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(columns = c(2:3), decimals = 0) |&gt;\n  gt::fmt_number(columns = c(4:7), decimals = 4) |&gt;\n  gt::fmt_percent(columns = 3, decimals = 0) |&gt;\n  gt::tab_spanner(\"Ratio\", starts_with(\"Ratio\")) |&gt;\n  gt::tab_spanner(\"Correlation\", starts_with(\"Corr\")) |&gt;\n  gt::cols_label(\n    sch.wide = \"Met Target\",\n    RatioEst = \"Est\",\n    RatioEst_se = \"SE\",\n    CorrEst = \"Est\",\n    CorrEst_se = \"SE\"\n  )\n\n\n\n\n\n\n\n\nMet Target\nMeals\nEnrollment\nRatio\nCorrelation\n\n\nEst\nSE\nEst\nSE\n\n\n\n\nNo\n58,038\n94,876,595%\n0.0612\n0.0096\n−0.1103\n0.1523\n\n\nYes\n251,724\n267,230,839%\n0.0942\n0.0062\n−0.0360\n0.0560\n\n\nNA\n309,762\n362,107,434%\n0.0855\n0.0055\n−0.0511\n0.0590\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeeringa, West, S. G. 2017. Applied Survey Data Analysis (2nd Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9781315153278.\n\n\nZimmer, Powell, S. A. 2024. Exploring Complex Survey Data Analysis Using r: A Tidy Introduction with Srvyr and Survey. Chapman & Hall: CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "06-statistical-tests.html",
    "href": "06-statistical-tests.html",
    "title": "6  Statistical Testing",
    "section": "",
    "text": "6.1 T-Test\nUse t-tests to compare two proportions or means. The difference between t-tests with non-survey data and survey data is based on the underlying variance estimation difference. survey::svyttest() handles sample weights.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Testing</span>"
    ]
  },
  {
    "objectID": "06-statistical-tests.html#t-test",
    "href": "06-statistical-tests.html#t-test",
    "title": "6  Statistical Testing",
    "section": "",
    "text": "ContinuousProportion\n\n\nTest whether api00 differs from 600.\n\nsvyttest(\n  formula = api00 - 600 ~ 0,\n  design = apisrs_des,\n  na.rm = TRUE\n)\n\n\n    Design-based one-sample t-test\n\ndata:  api00 - 600 ~ 0\nt = 6.1175, df = 198, p-value = 4.992e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 38.34439 74.82561\nsample estimates:\n  mean \n56.585 \n\n\n\n\nTest whether growth &gt; 0 on average.\n\nsvyttest(\n  formula = (growth &gt; 0) ~ 0,\n  design = apisrs_des,\n  na.rm = TRUE\n)\n\n\n    Design-based one-sample t-test\n\ndata:  (growth &gt; 0) ~ 0\nt = 39.781, df = 198, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.841129 0.928871\nsample estimates:\n mean \n0.885 \n\n\nTest whether growth is higher for high school than for others.\n\nsvyttest(\n  growth ~ (stype == \"H\"),\n  design = apisrs_des\n)\n\n\n    Design-based t-test\n\ndata:  growth ~ (stype == \"H\")\nt = -5.5339, df = 198, p-value = 9.845e-08\nalternative hypothesis: true difference in mean is not equal to 0\n95 percent confidence interval:\n -29.73118 -14.10882\nsample estimates:\ndifference in mean \n            -21.92",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Testing</span>"
    ]
  },
  {
    "objectID": "06-statistical-tests.html#chi-squared-test",
    "href": "06-statistical-tests.html#chi-squared-test",
    "title": "6  Statistical Testing",
    "section": "6.2 Chi-Squared Test",
    "text": "6.2 Chi-Squared Test\n\nGoodness of FitTest of IndependenceTest of Homogeneity\n\n\nDoes meals_cut distribution match hypothesized population [.25, .25, .5]?\n\ngof &lt;- svygofchisq(\n  formula = ~meals_cut,\n  p = c(.25, .25, .5),\n  design = apistrat_des,\n  na.rm = TRUE\n)\n\ngof\n\n\n    Design-based chi-squared test for given probabilities\n\ndata:  ~meals_cut\nX-squared = 1100.4, scale = 31.6017, df = 1.6449, p-value = 1.438e-08\n\n\n\n\nShow the code\napistrat_des |&gt; \n  summarize(\n    .by = hsg_cut,\n    observed = survey_mean(vartype = \"ci\")\n  ) |&gt;\n  mutate(expected = c(.25, .25, .5)) |&gt;\n  pivot_longer(c(observed, expected), values_to = \"Proportion\") |&gt;\n  ggplot(aes(x = hsg_cut)) +\n  geom_errorbar(aes(ymin = observed_low, ymax = observed_upp), width = .5) +\n  geom_point(aes(y = Proportion, color = name), size = 2) +\n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  labs(color = NULL)\n\n\n\n\n\n\n\n\n\n\n\nIs meals_cut distribution related to hsg_cut?\n\nsvychisq(\n  formula = ~ meals_cut + hsg_cut,\n  design = apistrat_des,\n  statistic = \"Wald\",\n  na.rm = TRUE\n)\n\n\n    Design-based Wald test of association\n\ndata:  NextMethod()\nF = 9.4761, ndf = 4, ddf = 197, p-value = 5.01e-07\n\n\ngtsummary does not have a cross table function, but you can make your own.\n\napistrat_des |&gt;\n  drop_na(meals_cut, hsg_cut) |&gt;\n  group_by(meals_cut, hsg_cut) |&gt;\n  summarize(Obs = round(survey_mean(vartype = \"ci\"), 3), .groups = \"drop\") |&gt;\n  mutate(prop = glue::glue(\"{Obs} ({Obs_low}, {Obs_upp})\")) |&gt;\n  pivot_wider(id_cols = meals_cut, names_from = hsg_cut, values_from = prop) |&gt;\n  gt::gt(rowname_col = \"Meals\") |&gt;\n  gt::tab_stubhead(\"High School Grad\")\n\n\n\n\n\n\n\nmeals_cut\n[0,12]\n(12,25]\n(25,100]\n\n\n\n\n[0,12]\n0.797 (0.642, 0.952)\n0.129 (0.007, 0.252)\n0.073 (-0.035, 0.182)\n\n\n(12,25]\n0.35 (0.175, 0.526)\n0.453 (0.278, 0.628)\n0.197 (0.06, 0.333)\n\n\n(25,100]\n0.127 (0.064, 0.189)\n0.374 (0.288, 0.46)\n0.499 (0.409, 0.59)\n\n\n\n\n\n\n\n\n\nIs the distribution of meals_cut the same for each level of hsg_cut?\n\nsvychisq(\n  formula = ~ meals_cut + hsg_cut,\n  design = apistrat_des,\n  statistic = \"Chisq\",\n  na.rm = TRUE\n)\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  NextMethod()\nX-squared = 59.835, df = 4, p-value = 1.63e-11",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Testing</span>"
    ]
  },
  {
    "objectID": "07-modeling.html",
    "href": "07-modeling.html",
    "title": "7  Modeling",
    "section": "",
    "text": "7.1 ANOVA\nANOVA tests whether the mean outcome is the same across groups. It is equivalent to the linear model. A full explanation of ANOVA is here, but with survey analysis, the equal variances assumption does not apply because of the weighting.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "07-modeling.html#anova",
    "href": "07-modeling.html#anova",
    "title": "7  Modeling",
    "section": "",
    "text": "PlotModel\n\n\n\napistrat |&gt;\n  ggplot(aes(x = hsg_cut, y = api00)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nsvyglm(\n  api00 ~ hsg_cut,\n  design = apistrat_des\n) |&gt;\n  gtsummary::tbl_regression(intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n759\n722, 795\n&lt;0.001\n\n\nhsg_cut\n\n\n\n\n\n\n\n\n    [0,12]\n—\n—\n\n\n\n\n    (12,25]\n-100\n-145, -56\n&lt;0.001\n\n\n    (25,100]\n-154\n-198, -109\n&lt;0.001\n\n\n\n1\nCI = Confidence Interval",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "07-modeling.html#linear-regression",
    "href": "07-modeling.html#linear-regression",
    "title": "7  Modeling",
    "section": "7.2 Linear Regression",
    "text": "7.2 Linear Regression\n\nPlotModel\n\n\n\napistrat |&gt;\n  ggplot(aes(x = api99, y = api00)) +\n  geom_point() +\n  facet_wrap(vars(hsg_cut))\n\n\n\n\n\n\n\n\n\n\n\nsvyglm(\n  api00 ~ api99 + hsg_cut,\n  design = apistrat_des\n) |&gt;\n  gtsummary::tbl_regression(intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n62\n35, 90\n&lt;0.001\n\n\napi99\n0.95\n0.91, 0.98\n&lt;0.001\n\n\nhsg_cut\n\n\n\n\n\n\n\n\n    [0,12]\n—\n—\n\n\n\n\n    (12,25]\n2.7\n-8.2, 14\n0.6\n\n\n    (25,100]\n7.1\n-4.4, 19\n0.2\n\n\n\n1\nCI = Confidence Interval",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "07-modeling.html#logistic-regression",
    "href": "07-modeling.html#logistic-regression",
    "title": "7  Modeling",
    "section": "7.3 Logistic Regression",
    "text": "7.3 Logistic Regression\n\nPlotModel\n\n\n\napistrat_des |&gt;\n  summarize(\n    .by = meals_cut,\n    met_pct = survey_mean(\n      sch.wide == \"Yes\", na.rm = TRUE, proportion = TRUE, vartype = \"ci\"\n    )\n  ) |&gt;\n  ggplot(aes(x = meals_cut, y = met_pct)) +\n  geom_col(width = .4) +\n  geom_errorbar(aes(ymin = met_pct_low, ymax = met_pct_upp), width = .1)\n\n\n\n\n\n\n\n\n\n\n\nsvyglm(\n  sch.wide ~ api99 + meals_cut,\n  design = apistrat_des,\n  family = quasibinomial\n) |&gt;\n  gtsummary::tbl_regression(intercept = TRUE) |&gt;\n  as_gt() |&gt;\n  gt::tab_caption(\"Logistic regression of meeting school-wide target by API in 1999.\")\n\n\n\nTable 7.1: Logistic regression of meeting school-wide target by API in 1999.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)\n1\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n-3.4\n-7.3, 0.41\n0.080\n\n\napi99\n0.01\n0.00, 0.01\n0.013\n\n\nmeals_cut\n\n\n\n\n\n\n\n\n    [0,12]\n—\n—\n\n\n\n\n    (12,25]\n0.87\n-0.49, 2.2\n0.2\n\n\n    (25,100]\n1.8\n0.13, 3.4\n0.034\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "08-best_worst_scaling.html",
    "href": "08-best_worst_scaling.html",
    "title": "8  Best-Worst Scaling",
    "section": "",
    "text": "8.1 Count Analysis\nYou can see below that respondent 1 selected item 1 (origin) Best three times, item 5 (safety) Best three times, and item 7 (milling) Best once. Respondent 1 selected items 2 (variety), 3 (price), 4 (taste), and 5 (safety) Worst once and item 6 (washfree) worst three times.\nbws %&gt;% \n  filter(id == 1, RES == TRUE) %&gt;%\n  gtsummary::tbl_cross(BEST, WORST)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWORST\n\nTotal\n\n\n2\n3\n4\n5\n6\n\n\n\n\nBEST\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n0\n1\n0\n1\n1\n3\n\n\n    5\n0\n0\n1\n0\n2\n3\n\n\n    7\n1\n0\n0\n0\n0\n1\n\n\nTotal\n1\n1\n1\n1\n3\n7\nbws.count() calculates counts for (b)est, (w)orst, best-minus-worst (bw), and standardized bw (sbw = bw / number of levels) for each item.\nbws_count &lt;- bws.count(bws, cl = 2)\ndim(bws_count)\n\n[1] 90 32\nbws_count %&gt;% filter(id == 1) %&gt;% glimpse()\n\nRows: 1\nColumns: 32\n$ id           &lt;dbl&gt; 1\n$ b.origin     &lt;dbl&gt; 3\n$ b.variety    &lt;dbl&gt; 0\n$ b.price      &lt;dbl&gt; 0\n$ b.taste      &lt;dbl&gt; 0\n$ b.safety     &lt;dbl&gt; 3\n$ b.washfree   &lt;dbl&gt; 0\n$ b.milling    &lt;dbl&gt; 1\n$ w.origin     &lt;dbl&gt; 0\n$ w.variety    &lt;dbl&gt; 1\n$ w.price      &lt;dbl&gt; 1\n$ w.taste      &lt;dbl&gt; 1\n$ w.safety     &lt;dbl&gt; 1\n$ w.washfree   &lt;dbl&gt; 3\n$ w.milling    &lt;dbl&gt; 0\n$ bw.origin    &lt;dbl&gt; 3\n$ bw.variety   &lt;dbl&gt; -1\n$ bw.price     &lt;dbl&gt; -1\n$ bw.taste     &lt;dbl&gt; -1\n$ bw.safety    &lt;dbl&gt; 2\n$ bw.washfree  &lt;dbl&gt; -3\n$ bw.milling   &lt;dbl&gt; 1\n$ sbw.origin   &lt;dbl&gt; 0.75\n$ sbw.variety  &lt;dbl&gt; -0.25\n$ sbw.price    &lt;dbl&gt; -0.25\n$ sbw.taste    &lt;dbl&gt; -0.25\n$ sbw.safety   &lt;dbl&gt; 0.5\n$ sbw.washfree &lt;dbl&gt; -0.75\n$ sbw.milling  &lt;dbl&gt; 0.25\n$ age          &lt;int&gt; 3\n$ hp           &lt;int&gt; 2\n$ chem         &lt;int&gt; 1\nplot() shows the relationship between the level means and standard deviations. Price, taste, and safety are similarly important, but price has a higher standard deviation, meaning its importance varies.\nplot(bws_count, score = \"bw\")\nThe column plot shows the item ranks.\nbws_count %&gt;%\n  select(id, starts_with(\"sbw\")) %&gt;%\n  pivot_longer(cols = -id) %&gt;%\n  group_by(name) %&gt;%\n  summarize(.groups = \"drop\", M = mean(value)) %&gt;%\n  arrange(M) %&gt;%\n  ggplot(aes(y = fct_inorder(name), x = M)) +\n  geom_col()",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Best-Worst Scaling</span>"
    ]
  },
  {
    "objectID": "08-best_worst_scaling.html#model",
    "href": "08-best_worst_scaling.html#model",
    "title": "8  Best-Worst Scaling",
    "section": "8.2 Model",
    "text": "8.2 Model\nFit a conditional logit model. A simple model uses the dummy vars, excluding one (washfree) to avoid singularity. The last term “- 1” means that the model has no alternative-specific constants. Use dfidx() to convert the data into a format appropriate for the model.\n\nfmla &lt;- RES ~ origin + variety + price + taste + safety + milling - 1\n\nbws_dfidx &lt;- dfidx(bws, idx = list(c(\"STR\", \"id\"), \"PAIR\"), choice = \"RES\")\n\nmlogit_fit &lt;- mlogit(formula = fmla, data = bws_dfidx)\n\nsummary(mlogit_fit)\n\n\nCall:\nmlogit(formula = RES ~ origin + variety + price + taste + safety + \n    milling - 1, data = bws_dfidx, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6        7        8 \n0.036508 0.095238 0.082540 0.119048 0.123810 0.122222 0.098413 0.053968 \n       9       10       11       12 \n0.147619 0.053968 0.030159 0.036508 \n\nnr method\n5 iterations, 0h:0m:0s \ng'(-H)^-1g = 8.32E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value  Pr(&gt;|z|)    \norigin   1.13096    0.11785  9.5969 &lt; 2.2e-16 ***\nvariety  1.10765    0.11615  9.5364 &lt; 2.2e-16 ***\nprice    2.01292    0.12565 16.0205 &lt; 2.2e-16 ***\ntaste    1.84700    0.12378 14.9213 &lt; 2.2e-16 ***\nsafety   2.07194    0.12622 16.4156 &lt; 2.2e-16 ***\nmilling  0.96028    0.11494  8.3546 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1318\n\n\nbws.sp() shows the shares of preference.\n\n# Specify the name of the base since it isn't in model.\n(bws_sp &lt;- bws.sp(mlogit_fit, base = \"washfree\"))\n\n    origin    variety      price      taste     safety    milling   washfree \n0.09835520 0.09608950 0.23759085 0.20126594 0.25203440 0.08292247 0.03174165 \n\n\nSafety was most important and was 0.252 / 0.238 = 1.0607917 times as important as the second place price.\nThis model isn’t a great fit, unfortunately. You cannot pull the McFadden’s R-squared easily, but the calculation is straight-forward.\n\nll0 &lt;- -90 * 7 * log(12) # log-likelihood at zero\nllb &lt;- as.numeric(mlogit_fit$logLik)\n1 - (llb/ll0) # McFadden's R-squared\n## [1] 0.1580881\n1 - ((llb-6)/ll0) # Adjusted McFadden's R-squared\n## [1] 0.1542554\n\nA possible improvement is the random parameters logit model.\n\nfmla_rp &lt;- RES ~ origin + variety + price + taste + safety + milling - 1 | 0\n\nmlogit_rp_fit &lt;- mlogit(\n  fmla_rp,\n  bws_dfidx,\n  rpar = c(origin = \"n\", variety = \"n\", price = \"n\", taste = \"n\", safety = \"n\", milling = \"n\"),\n  R = 100,\n  halton = NA,\n  panel = TRUE\n)\n\nsummary(mlogit_rp_fit)\n\n\nCall:\nmlogit(formula = RES ~ origin + variety + price + taste + safety + \n    milling - 1 | 0, data = bws_dfidx, rpar = c(origin = \"n\", \n    variety = \"n\", price = \"n\", taste = \"n\", safety = \"n\", milling = \"n\"), \n    R = 100, halton = NA, panel = TRUE)\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6        7        8 \n0.036508 0.095238 0.082540 0.119048 0.123810 0.122222 0.098413 0.053968 \n       9       10       11       12 \n0.147619 0.053968 0.030159 0.036508 \n\nbfgs method\n24 iterations, 0h:0m:8s \ng'(-H)^-1g = 9.32E-07 \ngradient close to zero \n\nCoefficients :\n           Estimate Std. Error z-value  Pr(&gt;|z|)    \norigin      1.74079    0.15489 11.2386 &lt; 2.2e-16 ***\nvariety     1.78710    0.15324 11.6624 &lt; 2.2e-16 ***\nprice       3.98883    0.24281 16.4277 &lt; 2.2e-16 ***\ntaste       3.09265    0.19980 15.4787 &lt; 2.2e-16 ***\nsafety      3.62784    0.19816 18.3078 &lt; 2.2e-16 ***\nmilling     1.63816    0.16366 10.0093 &lt; 2.2e-16 ***\nsd.origin   1.66733    0.17842  9.3451 &lt; 2.2e-16 ***\nsd.variety  1.86579    0.17869 10.4414 &lt; 2.2e-16 ***\nsd.price    2.68927    0.23760 11.3186 &lt; 2.2e-16 ***\nsd.taste    2.00738    0.18984 10.5740 &lt; 2.2e-16 ***\nsd.safety   1.95473    0.21113  9.2584 &lt; 2.2e-16 ***\nsd.milling  1.60534    0.18671  8.5983 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1093.1\n\nrandom coefficients\n        Min.   1st Qu.   Median     Mean  3rd Qu. Max.\norigin  -Inf 0.6161939 1.740794 1.740794 2.865394  Inf\nvariety -Inf 0.5286426 1.787098 1.787098 3.045554  Inf\nprice   -Inf 2.1749395 3.988827 3.988827 5.802715  Inf\ntaste   -Inf 1.7386952 3.092652 3.092652 4.446608  Inf\nsafety  -Inf 2.3093963 3.627839 3.627839 4.946282  Inf\nmilling -Inf 0.5553767 1.638164 1.638164 2.720951  Inf\n\n\nMcFadden’s R-squared increased substantially.\n\nllb_rp &lt;- as.numeric(mlogit_rp_fit$logLik)\n1 - (llb_rp/ll0) # McFadden's R-squared\n## [1] 0.3017665\n1 - ((llb_rp-6)/ll0) # Adjusted McFadden's R-squared\n## [1] 0.2979339\n\n\n# Specify the name of the base since it isn't in model.\n(bws_rp_sp &lt;- bws.sp(mlogit_rp_fit, base = \"washfree\", coef = items[-6]))\n\n     origin     variety       price       taste      safety     milling \n0.043367459 0.045422777 0.410650578 0.167597772 0.286218165 0.039137418 \n   washfree \n0.007605832 \n\n\nNow (surprisingly?), price is most important and was 0.411 / 0.286 = 1.4347467 times as important as the second place safety.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Best-Worst Scaling</span>"
    ]
  },
  {
    "objectID": "08-best_worst_scaling.html#footnotes",
    "href": "08-best_worst_scaling.html#footnotes",
    "title": "8  Best-Worst Scaling",
    "section": "",
    "text": "Notes are from http://lab.agr.hokudai.ac.jp/nmvr/03-bws1.html.↩︎",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Best-Worst Scaling</span>"
    ]
  },
  {
    "objectID": "11-other.html",
    "href": "11-other.html",
    "title": "9  Analysis",
    "section": "",
    "text": "9.1 Prepare the Data\nThe design object specifies the sampling design, weights, and other information. We’ll work with six datasets.\nThe survey package includes the Student performance in California schools data, api, a record of the Academic Performance Index based on standardized testing. api contains three datasets that illustrate design types.\ndata(api, package = \"survey\")\nNext is the American National Election Studies (ANES) dataset from the srvyrexploR package. It contain election surveys from 2020, anes_2020. anes_2020 is weighted to the sample, not the population. To make population inferences, ANES recommends using the Current Population Survey (CPS) to scale the weights up to the number of non-institutional U.S. citizens aged 18 or older living in the 50 U.S. states or D.C. in March 2020.1\ndata(anes_2020, package = \"srvyrexploR\")\n\n# Mar 2020, state-level population estimates.\ncps_state &lt;- censusapi::getCensus(\n  name = \"cps/basic/mar\",\n  vintage = 2020,\n  region = \"state\",\n  vars = c(\n    \"HRMONTH\", \"HRYEAR4\",  # month and year of interview\n    \"PRTAGE\", \"PRCITSHP\",  # age and citizenship\n    \"PWSSWGT\"  # final person-level weight.\n  ),\n  key = Sys.getenv(\"CENSUS_KEY\")\n)\n\n# Age 18+ with U.S. citizenship.\ntarget_pop &lt;- \n  cps_state |&gt; \n  mutate(across(everything(), as.numeric)) |&gt;\n  filter(PRTAGE &gt;= 18, PRCITSHP %in% c(1:4)) |&gt;\n  pull(PWSSWGT) |&gt;\n  sum()\n\n# Scale the individual person weights to the population of interest.\nanes &lt;- anes_2020 |&gt; mutate(Weight = V200010b / sum(V200010b) * target_pop)\nanes_2020 contains 7,453 rows. Because of stratification, the rows are weighted to account for over/under representation. Column V200010b is the full sample weight and ranges from 0.008 to 6.651 with a sum of 7,453. The last line of code scales the weights so that they sum to the CPS population value, 231,034,125.\nNext is the Residential Energy Consumption Survey (RECS) dataset, recs_2020. RECS is a study that measures energy consumption and expenditure in American households.\ndata(recs_2020, package = \"srvyrexploR\")\nLast is the National Health and Nutrition Examination Survey (NHANES). The survey collected 78 attributes from (n = 20,293) persons.\ndata(NHANESraw, package = \"NHANES\")\n\n# correction to weights\nNHANESraw &lt;- NHANESraw |&gt; mutate(WTMEC4YR = WTMEC2YR / 2)",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#prepare-the-data",
    "href": "11-other.html#prepare-the-data",
    "title": "9  Analysis",
    "section": "",
    "text": "apisrs is a simple random sample of (n = 200) schools,\napistrat is stratified sample of 3 school types (elementary, middle, high) with simple random sampling of different sizes in each stratum,\napiclus2 is a two-stage cluster sample of schools within districts.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#create-the-design-object",
    "href": "11-other.html#create-the-design-object",
    "title": "9  Analysis",
    "section": "9.2 Create the Design Object",
    "text": "9.2 Create the Design Object\nMost analysis is performed on a tbl_svy survey design object. You can use survey::svydesign() or the srvyr::as_survey_design() wrapper function. The srvyr package is usually preferable because it was designed with tidy principles.\nThere are two parameters in srvyr::as_survey_design() that reference columns in the data frame.\n\nPopulation weights, weights.\nFor an SRS design, the responses are equally weighted, so the column values should just be the population size divided by the sample size.\nFor a stratified design, the values should equal the sampled fraction of the strata population.\nFinite population correction, fpc. The FPC reduces the variance when a substantial fraction of the total population has been sampled. Set it to the stratum population size.\nSRS has no strata, so the column values should just be the population size.\nStratified designs should use the stratum population.\n\n\nSRSStratifiedTwo-stage ClusterStratified Cluster (ANES)Unstratified Cluster (RECS)4-stage (NHANES)\n\n\napisrs is a simple random sample of 200 schools from a population of 6,194 California schools. FPC column fpc all equal 6,194. Weights column pw all equal 6,194 / 200 = 30.97.\n\napisrs |&gt; count(pw, fpc) |&gt; knitr::kable()\n\n\n\n\npw\nfpc\nn\n\n\n\n\n30.97\n6194\n200\n\n\n\n\napisrs_des &lt;- as_survey_design(apisrs, weights = pw, fpc = fpc)\n\nsummary(apisrs_des)\n\nIndependent Sampling design\nCalled via srvyr\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03229 0.03229 0.03229 0.03229 0.03229 0.03229 \nPopulation size (PSUs): 6194 \nData variables:\n [1] \"cds\"      \"stype\"    \"name\"     \"sname\"    \"snum\"     \"dname\"   \n [7] \"dnum\"     \"cname\"    \"cnum\"     \"flag\"     \"pcttest\"  \"api00\"   \n[13] \"api99\"    \"target\"   \"growth\"   \"sch.wide\" \"comp.imp\" \"both\"    \n[19] \"awards\"   \"meals\"    \"ell\"      \"yr.rnd\"   \"mobility\" \"acs.k3\"  \n[25] \"acs.46\"   \"acs.core\" \"pct.resp\" \"not.hsg\"  \"hsg\"      \"some.col\"\n[31] \"col.grad\" \"grad.sch\" \"avg.ed\"   \"full\"     \"emer\"     \"enroll\"  \n[37] \"api.stu\"  \"pw\"       \"fpc\"     \n\n\n\n\napistrat is stratified on school type (stype): E = Elementary, M = Middle, and H = High School. Samples of 100/4421 (E), 50/1018 = (M), and 50/755 (H) determine the population weights, pw = fpc / n. Stratified designs require the strata parameter.\n\napistrat |&gt; \n  count(stype, pw, fpc) |&gt;\n  mutate(`pw*n` = pw * n) |&gt;\n  adorn_totals(,,,, -pw) |&gt;\n  knitr::kable()\n\n\n\n\nstype\npw\nfpc\nn\npw*n\n\n\n\n\nE\n44.2099990844727\n4421\n100\n4421\n\n\nH\n15.1000003814697\n755\n50\n755\n\n\nM\n20.3600006103516\n1018\n50\n1018\n\n\nTotal\n-\n6194\n200\n6194\n\n\n\n\napistrat_des &lt;- as_survey_design(apistrat, weights = pw, fpc = fpc, strata = stype)\n\nsummary(apistrat_des)\n\nStratified Independent Sampling design\nCalled via srvyr\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.02262 0.02262 0.03587 0.04014 0.05339 0.06623 \nStratum Sizes: \n             E  H  M\nobs        100 50 50\ndesign.PSU 100 50 50\nactual.PSU 100 50 50\nPopulation stratum sizes (PSUs): \n   E    H    M \n4421  755 1018 \nData variables:\n [1] \"cds\"      \"stype\"    \"name\"     \"sname\"    \"snum\"     \"dname\"   \n [7] \"dnum\"     \"cname\"    \"cnum\"     \"flag\"     \"pcttest\"  \"api00\"   \n[13] \"api99\"    \"target\"   \"growth\"   \"sch.wide\" \"comp.imp\" \"both\"    \n[19] \"awards\"   \"meals\"    \"ell\"      \"yr.rnd\"   \"mobility\" \"acs.k3\"  \n[25] \"acs.46\"   \"acs.core\" \"pct.resp\" \"not.hsg\"  \"hsg\"      \"some.col\"\n[31] \"col.grad\" \"grad.sch\" \"avg.ed\"   \"full\"     \"emer\"     \"enroll\"  \n[37] \"api.stu\"  \"pw\"       \"fpc\"     \n\n\n\n\napiclus2 is a two-level cluster design. First, 40 school districts (id dnum) were randomly selected from the 755 districts in the state (fpc1 = 757). Then a random sample of up to 5 schools (id snum) were sampled from the fpc2 schools in the districts. Clustered designs require the cluster ids from largest to smallest level. pw = fpc1 / 40.\n\napiclus2 |&gt; count(dnum, snum, pw, fpc1, fpc2) |&gt; \n  adorn_totals(,,,,,n) |&gt; DT::datatable()\n\n\n\n\napiclus_design &lt;- svydesign(\n  id = ~dnum + snum, # district id + school id\n  data = apiclus2, \n  weights = ~pw, \n  fpc = ~fpc1 + fpc2 # districts in state + schools in district\n)\n\nsummary(apiclus_design)\n\n2 - level Cluster Sampling design\nWith (40, 126) clusters.\nsvydesign(id = ~dnum + snum, data = apiclus2, weights = ~pw, \n    fpc = ~fpc1 + fpc2)\nProbabilities:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.003669 0.037743 0.052840 0.042390 0.052840 0.052840 \nPopulation size (PSUs): 757 \nData variables:\n [1] \"cds\"      \"stype\"    \"name\"     \"sname\"    \"snum\"     \"dname\"   \n [7] \"dnum\"     \"cname\"    \"cnum\"     \"flag\"     \"pcttest\"  \"api00\"   \n[13] \"api99\"    \"target\"   \"growth\"   \"sch.wide\" \"comp.imp\" \"both\"    \n[19] \"awards\"   \"meals\"    \"ell\"      \"yr.rnd\"   \"mobility\" \"acs.k3\"  \n[25] \"acs.46\"   \"acs.core\" \"pct.resp\" \"not.hsg\"  \"hsg\"      \"some.col\"\n[31] \"col.grad\" \"grad.sch\" \"avg.ed\"   \"full\"     \"emer\"     \"enroll\"  \n[37] \"api.stu\"  \"pw\"       \"fpc1\"     \"fpc2\"    \n\n\n\n\nSet nest=TRUE to nest clusters within the strata.\n\nanes_des &lt;- as_survey_design(\n    anes,\n    weights = Weight,\n    strata = V200010d,\n    ids = V200010c,\n    nest = TRUE\n  )\n\nanes_des\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (101) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: V200010c \n  - strata: V200010d \n  - weights: Weight \nData variables: \n  - V200001 (dbl), CaseID (dbl), V200002 (hvn_lbll), InterviewMode (fct),\n    V200010b (dbl), Weight (dbl), V200010c (dbl), VarUnit (fct), V200010d\n    (dbl), Stratum (fct), V201006 (hvn_lbll), CampaignInterest (fct), V201023\n    (hvn_lbll), EarlyVote2020 (fct), V201024 (hvn_lbll), V201025x (hvn_lbll),\n    V201028 (hvn_lbll), V201029 (hvn_lbll), V201101 (hvn_lbll), V201102\n    (hvn_lbll), VotedPres2016 (fct), V201103 (hvn_lbll),\n    VotedPres2016_selection (fct), V201228 (hvn_lbll), V201229 (hvn_lbll),\n    V201230 (hvn_lbll), V201231x (hvn_lbll), PartyID (fct), V201233 (hvn_lbll),\n    TrustGovernment (fct), V201237 (hvn_lbll), TrustPeople (fct), V201507x\n    (hvn_lbll), Age (dbl), AgeGroup (fct), V201510 (hvn_lbll), Education (fct),\n    V201546 (hvn_lbll), V201547a (hvn_lbll), V201547b (hvn_lbll), V201547c\n    (hvn_lbll), V201547d (hvn_lbll), V201547e (hvn_lbll), V201547z (hvn_lbll),\n    V201549x (hvn_lbll), RaceEth (fct), V201600 (hvn_lbll), Gender (fct),\n    V201607 (hvn_lbll), V201610 (hvn_lbll), V201611 (hvn_lbll), V201613\n    (hvn_lbll), V201615 (hvn_lbll), V201616 (hvn_lbll), V201617x (hvn_lbll),\n    Income (fct), Income7 (fct), V202051 (hvn_lbll), V202066 (hvn_lbll),\n    V202072 (hvn_lbll), VotedPres2020 (fct), V202073 (hvn_lbll), V202109x\n    (hvn_lbll), V202110x (hvn_lbll), VotedPres2020_selection (fct)\n\n\n\n\n\nrecs_des &lt;- as_survey_rep(\n  recs_2020,\n  weights = NWEIGHT,\n  repweights = NWEIGHT1:NWEIGHT60,\n  type = \"JK1\",\n  scale = 59 / 60,\n  mse = TRUE\n)\n\nrecs_des\n\nCall: Called via srvyr\nUnstratified cluster jacknife (JK1) with 60 replicates and MSE variances.\nSampling variables:\n  - repweights: `NWEIGHT1 + NWEIGHT2 + NWEIGHT3 + NWEIGHT4 + NWEIGHT5 +\n    NWEIGHT6 + NWEIGHT7 + NWEIGHT8 + NWEIGHT9 + NWEIGHT10 + NWEIGHT11 +\n    NWEIGHT12 + NWEIGHT13 + NWEIGHT14 + NWEIGHT15 + NWEIGHT16 + NWEIGHT17 +\n    NWEIGHT18 + NWEIGHT19 + NWEIGHT20 + NWEIGHT21 + NWEIGHT22 + NWEIGHT23 +\n    NWEIGHT24 + NWEIGHT25 + NWEIGHT26 + NWEIGHT27 + NWEIGHT28 + NWEIGHT29 +\n    NWEIGHT30 + NWEIGHT31 + NWEIGHT32 + NWEIGHT33 + NWEIGHT34 + NWEIGHT35 +\n    NWEIGHT36 + NWEIGHT37 + NWEIGHT38 + NWEIGHT39 + NWEIGHT40 + NWEIGHT41 +\n    NWEIGHT42 + NWEIGHT43 + NWEIGHT44 + NWEIGHT45 + NWEIGHT46 + NWEIGHT47 +\n    NWEIGHT48 + NWEIGHT49 + NWEIGHT50 + NWEIGHT51 + NWEIGHT52 + NWEIGHT53 +\n    NWEIGHT54 + NWEIGHT55 + NWEIGHT56 + NWEIGHT57 + NWEIGHT58 + NWEIGHT59 +\n    NWEIGHT60` \n  - weights: NWEIGHT \nData variables: \n  - DOEID (dbl), ClimateRegion_BA (fct), Urbanicity (fct), Region (fct),\n    REGIONC (chr), Division (fct), STATE_FIPS (chr), state_postal (fct),\n    state_name (fct), HDD65 (dbl), CDD65 (dbl), HDD30YR (dbl), CDD30YR (dbl),\n    HousingUnitType (fct), YearMade (ord), TOTSQFT_EN (dbl), TOTHSQFT (dbl),\n    TOTCSQFT (dbl), SpaceHeatingUsed (lgl), ACUsed (lgl), HeatingBehavior\n    (fct), WinterTempDay (dbl), WinterTempAway (dbl), WinterTempNight (dbl),\n    ACBehavior (fct), SummerTempDay (dbl), SummerTempAway (dbl),\n    SummerTempNight (dbl), NWEIGHT (dbl), NWEIGHT1 (dbl), NWEIGHT2 (dbl),\n    NWEIGHT3 (dbl), NWEIGHT4 (dbl), NWEIGHT5 (dbl), NWEIGHT6 (dbl), NWEIGHT7\n    (dbl), NWEIGHT8 (dbl), NWEIGHT9 (dbl), NWEIGHT10 (dbl), NWEIGHT11 (dbl),\n    NWEIGHT12 (dbl), NWEIGHT13 (dbl), NWEIGHT14 (dbl), NWEIGHT15 (dbl),\n    NWEIGHT16 (dbl), NWEIGHT17 (dbl), NWEIGHT18 (dbl), NWEIGHT19 (dbl),\n    NWEIGHT20 (dbl), NWEIGHT21 (dbl), NWEIGHT22 (dbl), NWEIGHT23 (dbl),\n    NWEIGHT24 (dbl), NWEIGHT25 (dbl), NWEIGHT26 (dbl), NWEIGHT27 (dbl),\n    NWEIGHT28 (dbl), NWEIGHT29 (dbl), NWEIGHT30 (dbl), NWEIGHT31 (dbl),\n    NWEIGHT32 (dbl), NWEIGHT33 (dbl), NWEIGHT34 (dbl), NWEIGHT35 (dbl),\n    NWEIGHT36 (dbl), NWEIGHT37 (dbl), NWEIGHT38 (dbl), NWEIGHT39 (dbl),\n    NWEIGHT40 (dbl), NWEIGHT41 (dbl), NWEIGHT42 (dbl), NWEIGHT43 (dbl),\n    NWEIGHT44 (dbl), NWEIGHT45 (dbl), NWEIGHT46 (dbl), NWEIGHT47 (dbl),\n    NWEIGHT48 (dbl), NWEIGHT49 (dbl), NWEIGHT50 (dbl), NWEIGHT51 (dbl),\n    NWEIGHT52 (dbl), NWEIGHT53 (dbl), NWEIGHT54 (dbl), NWEIGHT55 (dbl),\n    NWEIGHT56 (dbl), NWEIGHT57 (dbl), NWEIGHT58 (dbl), NWEIGHT59 (dbl),\n    NWEIGHT60 (dbl), BTUEL (dbl), DOLLAREL (dbl), BTUNG (dbl), DOLLARNG (dbl),\n    BTULP (dbl), DOLLARLP (dbl), BTUFO (dbl), DOLLARFO (dbl), BTUWOOD (dbl),\n    TOTALBTU (dbl), TOTALDOL (dbl)\n\n\n\n\nThe survey used a 4-stage design: stage 0 stratified the US by geography and proportion of minority populations; stage 1 randomly selected counties within strata; stage 2 randomly selected city blocks within counties; stage 3 randomly selected households within city blocks; and stage 4 randomly selected persons within households. When there are multiple levels of clusters like this, the convention is to assign the first cluster to ids. Set nest = TRUE because the cluster ids are nested within the strata (i.e., they are not unique).\n\nnhanes_des &lt;- as_survey_design(\n  NHANESraw, \n  strata = SDMVSTRA, \n  ids = SDMVPSU, \n  nest = TRUE, \n  weights = WTMEC4YR\n)\n\nsummary(nhanes_des)\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (62) clusters.\nCalled via srvyr\nProbabilities:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n8.986e-06 5.664e-05 1.054e-04       Inf 1.721e-04       Inf \nStratum Sizes: \n            75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\nobs        803 785 823 829 696 751 696 724 713 683 592 946 598 647 251 862 998\ndesign.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\nactual.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\n            92  93  94  95  96  97  98  99 100 101 102 103\nobs        875 602 688 722 676 608 708 682 700 715 624 296\ndesign.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nactual.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nData variables:\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeMonths\"        \"Race1\"            \"Race3\"            \"Education\"       \n [9] \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"      \"Poverty\"         \n[13] \"HomeRooms\"        \"HomeOwn\"          \"Work\"             \"Weight\"          \n[17] \"Length\"           \"HeadCirc\"         \"Height\"           \"BMI\"             \n[21] \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"            \"BPSysAve\"        \n[25] \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"           \"BPSys2\"          \n[29] \"BPDia2\"           \"BPSys3\"           \"BPDia3\"           \"Testosterone\"    \n[33] \"DirectChol\"       \"TotChol\"          \"UrineVol1\"        \"UrineFlow1\"      \n[37] \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"         \"DiabetesAge\"     \n[41] \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\"  \"LittleInterest\"  \n[45] \"Depressed\"        \"nPregnancies\"     \"nBabies\"          \"Age1stBaby\"      \n[49] \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"       \"PhysActiveDays\"  \n[53] \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"    \"CompHrsDayChild\" \n[57] \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"      \"SmokeNow\"        \n[61] \"Smoke100\"         \"SmokeAge\"         \"Marijuana\"        \"AgeFirstMarij\"   \n[65] \"RegularMarij\"     \"AgeRegMarij\"      \"HardDrugs\"        \"SexEver\"         \n[69] \"SexAge\"           \"SexNumPartnLife\"  \"SexNumPartYear\"   \"SameSex\"         \n[73] \"SexOrientation\"   \"WTINT2YR\"         \"WTMEC2YR\"         \"SDMVPSU\"         \n[77] \"SDMVSTRA\"         \"PregnantNow\"      \"WTMEC4YR\"        \n\n\nSurvey weights for minorities are typically lower because designers over-sample to get adequate representation. The weights sum to the sub-populations and the total population. I.e., WTMEC4YR = sub population size / sample size.\n\nNHANESraw |&gt;\n  summarize(.by = Race1, \n            `Sum(WTMEC4YR)` = sum(WTMEC4YR), \n            `Avg(WTMEC4YR)` = mean(WTMEC4YR), \n            n = n()) |&gt;\n  mutate(`Avg * n` = `Avg(WTMEC4YR)` * n) |&gt;\n  janitor::adorn_totals() |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 0)\n\n\n\n\n\n\n\nRace1\nSum(WTMEC4YR)\nAvg(WTMEC4YR)\nn\nAvg * n\n\n\n\n\nWhite\n193,966,274\n26,236\n7,393\n193,966,274\n\n\nOther\n23,389,002\n10,116\n2,312\n23,389,002\n\n\nBlack\n37,241,616\n8,026\n4,640\n37,241,616\n\n\nMexican\n30,719,158\n8,216\n3,739\n30,719,158\n\n\nHispanic\n18,951,150\n8,579\n2,209\n18,951,150\n\n\nTotal\n304,267,200\n61,174\n20,293\n304,267,200\n\n\n\n\n\n\n\nThe survey package functions handle the survey designs and weights. The population figures from the table above could have been built with svytable().\n\nsvytable(~Race1, design = nhanes_des) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(prop = Freq / sum(Freq) * 100) %&gt;%\n  arrange(desc(prop)) %&gt;%\n  adorn_totals() %&gt;%\n  flextable() %&gt;%\n  colformat_int(j = 2) %&gt;%\n  colformat_num(j = 3, suffix = \"%\", digits = 0)\n\nRace1FreqpropWhite193,966,27463.748664%Black37,241,61612.239773%Mexican30,719,15810.096112%Other23,389,0027.686994%Hispanic18,951,1506.228456%Total304,267,200100.000000%",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#descriptive-analysis",
    "href": "11-other.html#descriptive-analysis",
    "title": "9  Analysis",
    "section": "9.3 Descriptive Analysis",
    "text": "9.3 Descriptive Analysis\n\n9.3.1 Distributions\nCreate cross-tabs with survey_count().\n\napisrs_des |&gt; survey_count(stype)\n\n# A tibble: 3 × 3\n  stype     n  n_se\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 E     4398.  196.\n2 H      774.  143.\n3 M     1022.  160.\n\n# apisrs |&gt; \n#   summarize(.by = stype, n = sum(pw), n_se = sd(pw))\n#   count(Region, Division)\n\n\n\n9.3.2 Central Tendency\n\n\n9.3.3 Relationship\n\n\n9.3.4 Dispersion\nCreate a contingency table by including two variables in svytable(). Here is contingency table for self-reported health by depression expressed as a 100% stacked bar chart.\n\nsvytable(~Depressed + HealthGen, design = nhanes_des) %&gt;%\n  data.frame() %&gt;%\n  group_by(HealthGen) %&gt;%\n  mutate(n_HealthGen = sum(Freq), Prop_Depressed = Freq / sum(Freq)) %&gt;%\n  ggplot(aes(x = HealthGen, y = Prop_Depressed, fill = Depressed)) +\n  geom_col() + \n  coord_flip() +\n  theme_minimal() +\n  scale_fill_brewer()\n\n\n\n\n\n\n\n\nPerform a chi-square test of independence on contingency tables using the svychisq() function. Here is a test ofthe null hypothesis that depression is independent of general health.\n\nsvychisq(~Depressed + HealthGen, design = nhanes_des, statistic = \"Chisq\")\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  NextMethod()\nX-squared = 1592.7, df = 8, p-value &lt; 2.2e-16\n\n\nThe chi-square test with Rao & Scott adjustment is evidently not a standard chi-square test. Maybe in how it factors in survey design? The test statistic is usually \\(X^2 = \\sum (O - E)^2 / E.\\)\n\nO &lt;- svytable(~Depressed + HealthGen, design = nhanes_des) %&gt;% as.matrix()\nE &lt;- sum(O) * prop.table(O, 1) * prop.table(O, 2)\n(X2 &lt;- sum((O - E)^2 / E))\n## [1] 16025254\npchisq(X2, df = (nrow(O)-1) * (ncol(O) - 1), lower.tail = FALSE)\n## [1] 0\n\nwhich is what chisq.test() does.\n\nsvytable(~Depressed + HealthGen, design = nhanes_des) %&gt;% \n  as.matrix() %&gt;% \n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 16025254, df = 8, p-value &lt; 2.2e-16",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#exploring-quantitative-data",
    "href": "11-other.html#exploring-quantitative-data",
    "title": "9  Analysis",
    "section": "9.4 Exploring Quantitative Data",
    "text": "9.4 Exploring Quantitative Data\nThe svymean(), svytotal(), and svyquantile() functions summarize quantitative variables. To group by a factor variable, use svyby().\n\nsvyquantile(x = ~SleepHrsNight, \n            design = nhanes_des, \n            na.rm = TRUE, \n            quantiles = c(.01, .25, .50, .75, .99))\n\n$SleepHrsNight\n     quantile ci.2.5 ci.97.5        se\n0.01        4      4       5 0.2457588\n0.25        6      6       7 0.2457588\n0.5         7      7       8 0.2457588\n0.75        8      8       9 0.2457588\n0.99       10     10      11 0.2457588\n\nattr(,\"hasci\")\n[1] TRUE\nattr(,\"class\")\n[1] \"newsvyquantile\"\n\nsvymean(x = ~SleepHrsNight, design = nhanes_des, na.rm = TRUE)\n\n                mean     SE\nSleepHrsNight 6.9292 0.0166\n\n\n\nsvyby(formula = ~SleepHrsNight, by = ~Depressed, FUN = svymean, \n      design = nhanes_des, na.rm = TRUE, keep.names = FALSE) %&gt;%\n  ggplot(aes(x = Depressed, y = SleepHrsNight, \n             ymin = SleepHrsNight - 2*se, ymax = SleepHrsNight + 2*se)) +\n  geom_col(fill = \"lightblue\") +\n  geom_errorbar(width = 0.5)\n\n\n\n\n\n\n\n\nYou need raw data for the distribution plots, so be sure to weight the variables.\n\nNHANESraw %&gt;% \n  ggplot(aes(x = SleepHrsNight, weight = WTMEC4YR)) + \n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"#FFFFFF\", na.rm = TRUE)\n\n\n\n\n\n\n\nNHANESraw %&gt;% \n  filter(!is.na(SleepHrsNight) & !is.na(Gender)) %&gt;%\n  group_by(Gender) %&gt;%\n  mutate(WTMEC4YR_std = WTMEC4YR / sum(WTMEC4YR)) %&gt;%\n  ggplot(aes(x = SleepHrsNight, Weight = WTMEC4YR_std)) +\n  geom_density(bw = 0.6, fill = \"lightblue\") +\n  labs(x = \"Sleep Hours per Night\") +\n  facet_wrap(~Gender, labeller = \"label_both\")\n\n\n\n\n\n\n\n\nTest whether the population averages differ with a two-sample survey-weighted t-test. Use the svytest() function to incorporate the survey design.\n\nsvyttest(formula = SleepHrsNight ~ Gender, design = nhanes_des)\n\n\n    Design-based t-test\n\ndata:  SleepHrsNight ~ Gender\nt = -3.4077, df = 32, p-value = 0.001785\nalternative hypothesis: true difference in mean is not equal to 0\n95 percent confidence interval:\n -0.15506427 -0.03904047\nsample estimates:\ndifference in mean \n       -0.09705237",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#modeling-quantitative-data",
    "href": "11-other.html#modeling-quantitative-data",
    "title": "9  Analysis",
    "section": "9.5 Modeling Quantitative Data",
    "text": "9.5 Modeling Quantitative Data\nScatterplots need to adjust for the sampling weights. You can do this with the size or alpha aesthetics.\n\np1 &lt;- NHANESraw %&gt;% \n  filter(Age == 20) %&gt;%\n  ggplot(aes(x = Height, y = Weight, color = Gender, size = WTMEC4YR)) +\n  geom_jitter(width = 0.3, height = 0, alpha = 0.3) +\n  guides(size = FALSE) +\n  theme(legend.position = \"top\") +\n  labs(color = \"\")\n\np2 &lt;- NHANESraw %&gt;% \n  filter(Age == 20) %&gt;%\n  ggplot(aes(x = Height, y = Weight, color = Gender, alpha = WTMEC4YR)) +\n  geom_jitter(width = 0.3, height = 0) +\n  guides(alpha = FALSE) +\n  theme(legend.position = \"top\") +\n  labs(color = \"\")\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nFit a regression line with geom_smooth().\n\nNHANESraw %&gt;% \n  filter(!is.na(Weight) & !is.na(Height)) %&gt;%\n  ggplot(aes(x = Height, y = Weight, size = WTMEC4YR)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE, mapping = aes(weight = WTMEC4YR), \n              formula = y ~ x, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, mapping = aes(weight = WTMEC4YR), \n              formula = y ~ poly(x, 2), color = \"orange\") +\n  geom_smooth(method = \"lm\", se = FALSE, mapping = aes(weight = WTMEC4YR), \n              formula = y ~ poly(x, 3), color = \"red\") +\nguides(size = FALSE) \n\n\n\n\n\n\n\n\nModel a regression line with svyglm(). Let’s build a model to predict, BPSysAve, a person’s systolic blood pressure reading, using BPDiaAve, a person’s diastolic blood pressure reading and Diabetes, whether or not they were diagnosed with diabetes.\n\ndrop_na(NHANESraw, Diabetes, BPDiaAve, BPSysAve) %&gt;%\nggplot(mapping = aes(x = BPDiaAve, y = BPSysAve, size = WTMEC4YR, color = Diabetes)) + \n    geom_point(alpha = 0.2) + \n    guides(size = FALSE) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, mapping = aes(weight = WTMEC4YR))\n\n\n\n\n\n\n\nmod &lt;- svyglm(BPSysAve ~ BPDiaAve*Diabetes, design = nhanes_des)\nsummary(mod)\n\n\nCall:\nsvyglm(formula = BPSysAve ~ BPDiaAve * Diabetes, design = nhanes_des)\n\nSurvey design:\nCalled via srvyr\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          83.58652    2.05537  40.667  &lt; 2e-16 ***\nBPDiaAve              0.49964    0.02623  19.047  &lt; 2e-16 ***\nDiabetesYes          25.36616    3.56587   7.114 6.53e-08 ***\nBPDiaAve:DiabetesYes -0.22132    0.05120  -4.323 0.000156 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 279.1637)\n\nNumber of Fisher Scoring iterations: 2",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#surveyadministration",
    "href": "11-other.html#surveyadministration",
    "title": "9  Analysis",
    "section": "9.6 Survey Administration",
    "text": "9.6 Survey Administration\nThe second phase of a survey analysis is to collect the responses and perform an exploratory data analysis to familiarize yourself with the data.\n\n9.6.1 Frequencies\nbrand_rep is a brand reputation survey of n = 599 respondents answering nine 5-point Likert-scale items. The responses come in as numeric, and you will want to leave them that way for most analyses.\n\n#brand_rep &lt;- read_csv(url(\"https://assets.datacamp.com/production/repositories/4494/datasets/59b5f2d717ddd647415d8c88aa40af6f89ed24df/brandrep-cleansurvey-extraitem.csv\"))\n\nbrand_rep &lt;- read_csv(\"input/brand_rep.csv\")\n\npsych::response.frequencies(brand_rep)\n\n                       1         2          3           4          5 miss\nwell_made      0.6493739 0.2665474 0.04830054 0.014311270 0.02146691    0\nconsistent     0.6779964 0.2343470 0.04651163 0.019677996 0.02146691    0\npoor_workman_r 0.7584973 0.1699463 0.04472272 0.007155635 0.01967800    0\nhigher_price   0.3130590 0.2593918 0.18425760 0.119856887 0.12343470    0\nlot_more       0.1484794 0.1735242 0.15563506 0.162790698 0.35957066    0\ngo_up          0.1842576 0.1824687 0.19677996 0.177101968 0.25939177    0\nstands_out     0.4275492 0.3041145 0.13953488 0.067978533 0.06082290    0\nunique         0.4579606 0.2933810 0.11985689 0.069767442 0.05903399    0\none_of_a_kind  0.6225403 0.1949911 0.08944544 0.023255814 0.06976744    0\n\n\nSummarize Likert response with the likert::likert() function. This is the one place where you will need the items to be treated as factors.\n\nbrand_rep %&gt;%\n  data.frame() %&gt;% # read_csv() returns a tibble\n  mutate(across(everything(), as.factor)) %&gt;%  # likert() uses factors\n  likert::likert() %&gt;%\n  plot() + \n  labs(title = \"Brand Reputation Survey\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nMissing values may mean respondents did not understand the question or did not want to reveal their answer. If &lt;5% of survey responses have no missing values, you can just drop those responses. If missing values are a problem, try the Hmisc::naclus() to see which items tend to be missing in the same record. This survey is clean.\n\nnrow(brand_rep) - nrow(na.omit(brand_rep)) # num cases\n## [1] 0\ncolSums(is.na(brand_rep)) # num cases by col\n##      well_made     consistent poor_workman_r   higher_price       lot_more \n##              0              0              0              0              0 \n##          go_up     stands_out         unique  one_of_a_kind \n##              0              0              0              0\n\n\n\n9.6.2 Correlations\nYou will want to identify items that correlate highly with each other, but not highly outside their group. These patterns are the basis of mapping factors to the latent variables. Factors are the concrete survey items; latent variables are the abstract concepts they are intended to supply, like brand loyalty or customer satisfaction. The correlation plot below appears to have 3 groups, plus a stand-alone variable (one_of_a_kind).\n\n#psych::corr.test(brand_rep)\ncorrplot::corrplot(cor(brand_rep), method = \"circle\")",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#reporting",
    "href": "11-other.html#reporting",
    "title": "9  Analysis",
    "section": "9.7 Reporting",
    "text": "9.7 Reporting\nThere are seven key areas to report:\n\nExplain the study objective, explicitly identifying the research question.\nMotivate the research in the context of previous work.\nExplain the method and rationale, including the instrument and its psychometric properties, it development/testing, sample selection, and data collection. Explain and justify the analytical methods.\nPresent the results in a concise and factual manner.\nInterpret and discuss the findings.\nDraw conclusions.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "11-other.html#footnotes",
    "href": "11-other.html#footnotes",
    "title": "9  Analysis",
    "section": "",
    "text": "To pull data from the U.S. Census, visit their API page to request a key (free and fast). Create a .Renviron file in your RStudio project directory with CENSUS_KEY=\"&lt;your assigned key&gt;\". Make sure .Renviron is not tracked in GitHub by adding .Renviron to the .gitignore file.↩︎",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "09-case_study_nhanes.html",
    "href": "09-case_study_nhanes.html",
    "title": "10  Case Study: NHANES",
    "section": "",
    "text": "10.1 Executive summary",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Case Study: NHANES</span>"
    ]
  },
  {
    "objectID": "09-case_study_nhanes.html#executive-summary",
    "href": "09-case_study_nhanes.html#executive-summary",
    "title": "10  Case Study: NHANES",
    "section": "",
    "text": "Provide a brief overview of the survey including information related to general study goals and year when annual survey was first implemented.\nDescribe the purpose of this document.\nProvide a table of the sample size to be selected per business unit (i.e., respondent sample size inflated for ineligibility and nonresponse).\nDiscuss the contents of the remaining section of the report.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Case Study: NHANES</span>"
    ]
  },
  {
    "objectID": "09-case_study_nhanes.html#sample-design",
    "href": "09-case_study_nhanes.html#sample-design",
    "title": "10  Case Study: NHANES",
    "section": "10.2 Sample design",
    "text": "10.2 Sample design\nDescription of the target population.\nThe target population was adults aged 18 and older residing in the 48 contiguous United States.\nDescribe the sampling frame including the date and source database.\nThe sampling frame included households in the 48 contiguous United States. The survey was conducted between February 2001 and April 2003. The source database for the sampling frame was the Inter-university Consortium for Political and Social Research (ICPSR), which provided the necessary geographic and demographic information to ensure a nationally representative sample.\nDescribe the type of sample and method of sample selection to be used.\nThe sampling strategy was a multi-stage clustered area probability sample. - Stage 1 was to select primary sampling units. The entire country (48 contiguous states) was divided into primary sampling units (PSUs), composed of counties or groups of contiguous counties. A random sample of PSUs was selected. - Stage 2 was to select segments. The selected PSUs were subdivided into segments, usually census tracts or block groups. A random sample of segments was selected. - Stage 3 was to select households. A random sample of households was selected. - Stage 4 was to select respondents. A person was randomly selected from the household.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Case Study: NHANES</span>"
    ]
  },
  {
    "objectID": "09-case_study_nhanes.html#sample-size-and-allocation",
    "href": "09-case_study_nhanes.html#sample-size-and-allocation",
    "title": "10  Case Study: NHANES",
    "section": "10.3 Sample size and allocation",
    "text": "10.3 Sample size and allocation\n\nOptimization requirements – Optimization details including constraints and budget.\n– Detail the minimum domain sizes and mechanics used to determine the sizes.\nOptimization results\n– Results: minimum respondent sample size per stratum\n– Marginal sample sizes for key reporting domains\n– Estimated precision achieved by optimization results\n\nInflation adjustments to allocation solution\n– Nonresponse adjustments\n– Adjustments for ineligible sample members\n\nFinal sample allocation\n– Marginal sample sizes for key reporting domains\n\nSensitivity analysis\n– Results from comparing deviations to allocation after introducing changes to the optimization system\n\n\n# Downloaded from book web site\n# https://websites.umich.edu/~surveymethod/asda/#Links%20to%20Data%20Sets%20for%20First%20and%20Second%20Editions\n# https://www.umich.edu/~surveymethod/asda/Chapter%20Exercises%20Data%20Sets%20Stata%2015SEP2017.zip\nnhanes &lt;- foreign::read.dta(\"input/nhanes1112_sub_10jun2016.dta\")\n\nnhanes_des &lt;- as_survey_design(\n  nhanes,\n  ids = sdmvpsu,\n  strata = sdmvstra,\n  nest = TRUE\n)",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Case Study: NHANES</span>"
    ]
  },
  {
    "objectID": "09-case_study_nhanes.html#descriptive-analysis",
    "href": "09-case_study_nhanes.html#descriptive-analysis",
    "title": "10  Case Study: NHANES",
    "section": "10.4 Descriptive Analysis",
    "text": "10.4 Descriptive Analysis\n\n10.4.1 Counts\nHow many U.S. adults have experience irregular heartbeats in their lifetime?\n\nnhanes_des |&gt;\n  survey_count(.by = irregular, vartype = c(\"se\", \"ci\", \"cv\")) |&gt;\n  adorn_totals(, fill = NA,,, n) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(n:n_upp, decimals = 0) |&gt;\n  gt::fmt_number(n_cv, decimals = 2) |&gt;\n  gt::cols_label(\n    n = \"Estimted Total Lifetime MDE\",\n    n_se = \"Standard Error\",\n    n_low = \"95% CI (low)\",\n    n_upp = \"95% CI (upp)\",\n    n_cv = \"CV\"\n  )\n\n\n\n\n\n\n\n.by\nEstimted Total Lifetime MDE\nStandard Error\n95% CI (low)\n95% CI (upp)\nCV\n\n\n\n\n0\n8,906\n185\n8,516\n9,296\n0.02\n\n\n1\n121\n11\n97\n145\n0.09\n\n\nNA\n729\n45\n635\n823\n0.06\n\n\nTotal\n9,756\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n10.4.2 Sums\nWhat is the total number of participated at 18 or over by educational category? Sum age18p.\n\nnhanes_des |&gt;\n  summarize(\n    .by = edcat, \n    Tot = survey_total(age18p, na.rm = TRUE, vartype = c(\"se\", \"ci\", \"var\", \"cv\"))\n  ) |&gt;\n  adorn_totals(, fill = NA,,, Tot, Tot) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(Tot:Tot_var, decimals = 0) |&gt;\n  gt::fmt_number(Tot_cv, decimals = 2)\n\n\n\n\n\n\n\nedcat\nTot\nTot_se\nTot_low\nTot_upp\nTot_var\nTot_cv\n\n\n\n\n1\n1,443\n102\n1,227\n1,659\n10,485\n0.07\n\n\n2\n1,258\n47\n1,159\n1,357\n2,222\n0.04\n\n\n3\n1,761\n99\n1,553\n1,969\n9,734\n0.06\n\n\n4\n1,397\n119\n1,146\n1,648\n14,180\n0.09\n\n\nNA\n5\n2\n0\n10\n5\n0.45\n\n\nTotal\n5,864\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n10.4.3 Means and Proportions\nWhat was the mean systolic blood pressure by marital status? Calculate mean(BPXSY1).\n\nnhanes_des |&gt;\n  summarize(\n    .by = marcat,\n    M = survey_mean(BPXSY1, na.rm = TRUE, vartype = c(\"se\", \"ci\"))\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 0)\n\n\n\n\n\n\n\nmarcat\nM\nM_se\nM_low\nM_upp\n\n\n\n\n1\n124\n1\n122\n125\n\n\n2\n130\n1\n128\n133\n\n\n3\n119\n1\n118\n121\n\n\nNA\n106\n0\n105\n107\n\n\n\n\n\n\n\nWhat is the probability of holding each marital status? Calculate the proportion of marcat.\n\nnhanes_des |&gt;\n  summarize(\n    .by = marcat,\n    M = survey_prop()\n  ) |&gt;\n  adorn_totals(, fill = NA,,, M) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(M:M_se, decimals = 3)\n\n\n\n\n\n\n\nmarcat\nM\nM_se\n\n\n\n\n1\n0.320\n0.013\n\n\n2\n0.127\n0.005\n\n\n3\n0.122\n0.013\n\n\nNA\n0.431\n0.010\n\n\nTotal\n1.000\nNA\n\n\n\n\n\n\n\n\n\n10.4.4 Quantiles\nWhat was the IQR of systolic blood pressure by marital status? Calculate the IQR of BPXSY1.\n\nnhanes_des |&gt;\n  summarize(\n    .by = marcat,\n    Q = survey_quantile(BPXSY1, quantiles = c(.25, .5, .75))\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(ends_with(\"se\"), decimals = 2)\n\n\n\n\n\n\n\nmarcat\nQ_q25\nQ_q50\nQ_q75\nQ_q25_se\nQ_q50_se\nQ_q75_se\n\n\n\n\n1\n114\n124\n142\nNaN\nNaN\nNaN\n\n\n2\n116\n132\n154\nNaN\nNaN\nNaN\n\n\n3\n110\n120\n134\nNaN\nNaN\nNaN\n\n\nNA\n108\nNA\nNA\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n10.4.5 Ratios\nEstimate the ratio of high-density total cholesterol.\n\nnhanes_des |&gt; \n  filter(age &gt;= 18) |&gt;\n  summarize(\n    n = survey_total(),\n    R = survey_ratio(lbdhdd, lbxtc, na.rm = TRUE, vartype = c(\"se\", \"ci\"))\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(everything(), decimals = 3)\n\n\n\n\n\n\n\nn\nn_se\nR\nR_se\nR_low\nR_upp\n\n\n\n\n5,864.000\n125.028\n0.273\n0.003\n0.267\n0.280\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeeringa, West, S. G. 2017. Applied Survey Data Analysis (2nd Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9781315153278.\n\n\nValliant, Jill A.; Kreuter, Richard; Dever. 2013. Practical Tools for Designing and Weighting Survey Samples. Springer. https://doi.org/10.1007/978-3-319-93632-1.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Case Study: NHANES</span>"
    ]
  },
  {
    "objectID": "10-case_study_ncsr.html",
    "href": "10-case_study_ncsr.html",
    "title": "11  Case Study: NCS-R",
    "section": "",
    "text": "11.1 Executive summary",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Study: NCS-R</span>"
    ]
  },
  {
    "objectID": "10-case_study_ncsr.html#executive-summary",
    "href": "10-case_study_ncsr.html#executive-summary",
    "title": "11  Case Study: NCS-R",
    "section": "",
    "text": "Provide a brief overview of the survey including information related to general study goals and year when annual survey was first implemented.\nDescribe the purpose of this document.\nProvide a table of the sample size to be selected per business unit (i.e., respondent sample size inflated for ineligibility and nonresponse).\nDiscuss the contents of the remaining section of the report.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Study: NCS-R</span>"
    ]
  },
  {
    "objectID": "10-case_study_ncsr.html#sample-design",
    "href": "10-case_study_ncsr.html#sample-design",
    "title": "11  Case Study: NCS-R",
    "section": "11.2 Sample design",
    "text": "11.2 Sample design\nDescription of the target population.\nThe target population was adults aged 18 and older residing in the 48 contiguous United States.\nDescribe the sampling frame including the date and source database.\nThe sampling frame included households in the 48 contiguous United States. The survey was conducted between February 2001 and April 2003. The source database for the sampling frame was the Inter-university Consortium for Political and Social Research (ICPSR), which provided the necessary geographic and demographic information to ensure a nationally representative sample.\nDescribe the type of sample and method of sample selection to be used.\nThe sampling strategy was a multi-stage clustered area probability sample. - Stage 1 was to select primary sampling units. The entire country (48 contiguous states) was divided into primary sampling units (PSUs), composed of counties or groups of contiguous counties. A random sample of PSUs was selected. - Stage 2 was to select segments. The selected PSUs were subdivided into segments, usually census tracts or block groups. A random sample of segments was selected. - Stage 3 was to select households. A random sample of households was selected. - Stage 4 was to select respondents. A person was randomly selected from the household.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Study: NCS-R</span>"
    ]
  },
  {
    "objectID": "10-case_study_ncsr.html#sample-size-and-allocation",
    "href": "10-case_study_ncsr.html#sample-size-and-allocation",
    "title": "11  Case Study: NCS-R",
    "section": "11.3 Sample size and allocation",
    "text": "11.3 Sample size and allocation\n\nOptimization requirements – Optimization details including constraints and budget.\n– Detail the minimum domain sizes and mechanics used to determine the sizes.\nOptimization results\n– Results: minimum respondent sample size per stratum\n– Marginal sample sizes for key reporting domains\n– Estimated precision achieved by optimization results\n\nInflation adjustments to allocation solution\n– Nonresponse adjustments\n– Adjustments for ineligible sample members\n\nFinal sample allocation\n– Marginal sample sizes for key reporting domains\n\nSensitivity analysis\n– Results from comparing deviations to allocation after introducing changes to the optimization system\n\n\n# Downloaded from book web site\n# https://websites.umich.edu/~surveymethod/asda/#Links%20to%20Data%20Sets%20for%20First%20and%20Second%20Editions\n# https://www.umich.edu/~surveymethod/asda/Chapter%20Exercises%20Data%20Sets%20Stata%2015SEP2017.zip\nncsr_raw &lt;- foreign::read.dta(\"input/ncsr_sub_13nov2015.dta\")\n\nncsr &lt;- ncsr_raw |&gt; mutate(ncsrwtsh_pop = ncsrwtsh * (209128094 / 9282))\n\nncsr_des &lt;- as_survey_design(\n  ncsr,\n  ids = seclustr,\n  strata = sestrat,\n  nest = TRUE,\n  weights = ncsrwtsh_pop\n)",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Study: NCS-R</span>"
    ]
  },
  {
    "objectID": "10-case_study_ncsr.html#descriptive-analysis",
    "href": "10-case_study_ncsr.html#descriptive-analysis",
    "title": "11  Case Study: NCS-R",
    "section": "11.4 Descriptive Analysis",
    "text": "11.4 Descriptive Analysis\n\n11.4.1 Counts\nHow many U.S. adults have experienced an episode of major depression in their lifetime?\n\nncsr_des |&gt;\n  survey_count(.by = mde, vartype = c(\"se\", \"ci\", \"cv\")) |&gt;\n  adorn_totals(, fill = NA,,, n) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(n:n_upp, decimals = 0) |&gt;\n  gt::fmt_number(n_cv, decimals = 2) |&gt;\n  gt::cols_label(\n    n = \"Estimted Total Lifetime MDE\",\n    n_se = \"Standard Error\",\n    n_low = \"95% CI (low)\",\n    n_upp = \"95% CI (upp)\",\n    n_cv = \"CV\"\n  )\n\n\n\n\n\n\n\n.by\nEstimted Total Lifetime MDE\nStandard Error\n95% CI (low)\n95% CI (upp)\nCV\n\n\n\n\n0\n169,035,891\n7,876,170\n153,141,136\n184,930,645\n0.05\n\n\n1\n40,092,207\n2,567,488\n34,910,806\n45,273,607\n0.06\n\n\nTotal\n209,128,097\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nHow many U.S. adults have experienced an episode of major depression in their lifetime by marital status subpopulation?\n\nncsr_des |&gt;\n  filter(mde == 1) |&gt;\n  survey_count(.by = MAR3CAT, vartype = c(\"se\", \"ci\", \"cv\")) |&gt;\n  adorn_totals(, fill = NA,,, n) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(n:n_upp, decimals = 0) |&gt;\n  gt::fmt_number(n_cv, decimals = 2) |&gt;\n  gt::cols_label(\n    n = \"Estimted Total Lifetime MDE\",\n    n_se = \"Standard Error\",\n    n_low = \"95% CI (low)\",\n    n_upp = \"95% CI (upp)\",\n    n_cv = \"CV\"\n  )\n\n\n\n\n\n\n\n.by\nEstimted Total Lifetime MDE\nStandard Error\n95% CI (low)\n95% CI (upp)\nCV\n\n\n\n\n1\n20,304,191\n1,584,109\n17,107,330\n23,501,051\n0.08\n\n\n2\n10,360,671\n702,622\n8,942,723\n11,778,618\n0.07\n\n\n3\n9,427,345\n773,138\n7,867,091\n10,987,600\n0.08\n\n\nTotal\n40,092,207\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n11.4.2 Sums\nWhat is the total number of females by obesity category? Sum sexf.\n\nncsr_des |&gt;\n  summarize(\n    .by = OBESE6CA, \n    Tot = survey_total(sexf, na.rm = TRUE, vartype = c(\"se\", \"ci\", \"var\", \"cv\"))\n  ) |&gt;\n  adorn_totals(, fill = NA,,, Tot, Tot) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(Tot:Tot_var, decimals = 0) |&gt;\n  gt::fmt_number(Tot_cv, decimals = 2)\n\n\n\n\n\n\n\nOBESE6CA\nTot\nTot_se\nTot_low\nTot_upp\nTot_var\nTot_cv\n\n\n\n\n1\n6,131,462\n535,122\n5,051,541\n7,211,382\n286,355,856,271\n0.09\n\n\n2\n45,404,619\n2,720,611\n39,914,204\n50,895,035\n7,401,724,264,861\n0.06\n\n\n3\n28,849,013\n1,527,843\n25,765,701\n31,932,324\n2,334,302,987,994\n0.05\n\n\n4\n15,796,359\n857,378\n14,066,100\n17,526,617\n735,096,749,984\n0.05\n\n\n5\n6,490,138\n584,221\n5,311,132\n7,669,145\n341,314,438,641\n0.09\n\n\n6\n4,332,572\n451,013\n3,422,390\n5,242,753\n203,412,969,143\n0.10\n\n\nNA\n1,982,485\n222,624\n1,533,211\n2,431,759\n49,561,583,033\n0.11\n\n\nTotal\n108,986,647\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n11.4.3 Means and Proportions\nWhat was the mean age by region? Calculate mean(age).\n\nncsr_des |&gt;\n  summarize(\n    .by = region,\n    M = survey_mean(age, na.rm = TRUE, vartype = c(\"se\", \"ci\"))\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(decimals = 0)\n\n\n\n\n\n\n\nregion\nM\nM_se\nM_low\nM_upp\n\n\n\n\n1\n46\n1\n44\n48\n\n\n2\n45\n0\n44\n46\n\n\n3\n45\n0\n44\n46\n\n\n4\n43\n1\n41\n45\n\n\n\n\n\n\n\nWhat is the proportion of respondents from each reason? Calculate the proportion of region.\n\nncsr_des |&gt;\n  summarize(\n    .by = region,\n    M = survey_prop()\n  ) |&gt;\n  adorn_totals(, fill = NA,,, M) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(M:M_se, decimals = 3)\n\n\n\n\n\n\n\nregion\nM\nM_se\n\n\n\n\n1\n0.193\n0.033\n\n\n2\n0.232\n0.018\n\n\n3\n0.358\n0.020\n\n\n4\n0.217\n0.022\n\n\nTotal\n1.000\nNA\n\n\n\n\n\n\n\n\n\n11.4.4 Quantiles\nWhat is the IQR of the age by region? Calculate the quantiles of age.\n\nncsr_des |&gt;\n  summarize(\n    .by = region,\n    Q = survey_quantile(age, quantiles = c(.25, .5, .75))\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(ends_with(\"se\"), decimals = 2)\n\n\n\n\n\n\n\nregion\nQ_q25\nQ_q50\nQ_q75\nQ_q25_se\nQ_q50_se\nQ_q75_se\n\n\n\n\n1\n33\n44\n58\n1.33\n1.33\n1.33\n\n\n2\n31\n43\n57\n0.68\n0.68\n0.68\n\n\n3\n30\n43\n57\n0.70\n0.70\n0.70\n\n\n4\n28\n41\n54\n2.60\n1.30\n1.30\n\n\n\n\n\n\n\n\n\n11.4.5 Ratios\nWhat is the ratio of age to DSM_SO?\n\nncsr_des |&gt;\n  summarize(\n    .by = region,\n    R = survey_ratio(age, DSM_SO)\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(ends_with(\"se\"), decimals = 2)\n\n\n\n\n\n\n\nregion\nR\nR_se\n\n\n\n\n1\n10.097784\n0.21\n\n\n2\n10.061276\n0.06\n\n\n3\n9.826059\n0.11\n\n\n4\n9.736837\n0.28\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeeringa, West, S. G. 2017. Applied Survey Data Analysis (2nd Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9781315153278.\n\n\nValliant, Jill A.; Kreuter, Richard; Dever. 2013. Practical Tools for Designing and Weighting Survey Samples. Springer. https://doi.org/10.1007/978-3-319-93632-1.",
    "crumbs": [
      "Case Studies",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Study: NCS-R</span>"
    ]
  },
  {
    "objectID": "12-references.html",
    "href": "12-references.html",
    "title": "References",
    "section": "",
    "text": "Heeringa, West, S. G. 2017. Applied Survey Data Analysis (2nd\nEd.). Chapman; Hall/CRC. https://doi.org/10.1201/9781315153278.\n\n\nMiddleton, Fiona. 2022. “Reliability Vs. Validity in Research |\nDifference, Types and Examples.” https://www.scribbr.com/methodology/reliability-vs-validity/.\n\n\nMount, George. n.d. “Survey and Measurement Development in\nr.” https://app.datacamp.com/learn/courses/survey-and-measurement-development-in-r.\n\n\nValliant, Jill A.; Kreuter, Richard; Dever. 2013. Practical Tools\nfor Designing and Weighting Survey Samples. Springer. https://doi.org/10.1007/978-3-319-93632-1.\n\n\nZimmer, Powell, S. A. 2024. Exploring Complex Survey Data Analysis\nUsing r: A Tidy Introduction with Srvyr and Survey. Chapman &\nHall: CRC Press. https://tidy-survey-r.github.io/tidy-survey-book/.",
    "crumbs": [
      "References"
    ]
  }
]